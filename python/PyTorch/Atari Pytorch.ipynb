{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari Pytorch Convolutional Network Breakdown\n",
    "<br><strong>\n",
    "Michael Ruggiero<br>\n",
    "michael@mcruggiero.com<br>\n",
    "Tuesday, August 13th, 2019<br></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Full-Code\" data-toc-modified-id=\"Full-Code-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Full Code</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T15:44:41.695325Z",
     "start_time": "2019-08-13T13:13:41.513681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: -20.40, [last 100]: -2.24, [all]: -20.36                       \n",
      "epsilon: 0.34, frames_total: 9928\n",
      "Elapsed time:  00:01:11\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: -20.20, [last 100]: -4.26, [all]: -20.29                       \n",
      "epsilon: 0.14, frames_total: 19052\n",
      "Elapsed time:  00:02:16\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: -20.10, [last 100]: -6.27, [all]: -20.23                       \n",
      "epsilon: 0.06, frames_total: 29110\n",
      "Elapsed time:  00:03:34\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: -18.00, [last 100]: -8.07, [all]: -19.68                       \n",
      "epsilon: 0.02, frames_total: 45686\n",
      "Elapsed time:  00:05:38\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: -16.10, [last 100]: -9.68, [all]: -18.98                       \n",
      "epsilon: 0.01, frames_total: 66738\n",
      "Elapsed time:  00:08:23\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: -15.50, [last 100]: -11.23, [all]: -18.41                       \n",
      "epsilon: 0.01, frames_total: 87303\n",
      "Elapsed time:  00:10:58\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: -14.20, [last 100]: -12.65, [all]: -17.82                       \n",
      "epsilon: 0.01, frames_total: 108969\n",
      "Elapsed time:  00:13:43\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: -13.10, [last 100]: -13.96, [all]: -17.23                       \n",
      "epsilon: 0.01, frames_total: 131515\n",
      "Elapsed time:  00:16:40\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: -7.10, [last 100]: -14.67, [all]: -16.12                       \n",
      "epsilon: 0.01, frames_total: 158437\n",
      "Elapsed time:  00:20:05\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: -10.40, [last 100]: -15.51, [all]: -15.55                       \n",
      "epsilon: 0.01, frames_total: 182188\n",
      "Elapsed time:  00:23:04\n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: -7.60, [last 100]: -14.23, [all]: -14.84                       \n",
      "epsilon: 0.01, frames_total: 206480\n",
      "Elapsed time:  00:26:06\n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: -2.50, [last 100]: -12.46, [all]: -13.82                       \n",
      "epsilon: 0.01, frames_total: 233987\n",
      "Elapsed time:  00:29:41\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: -4.10, [last 100]: -10.86, [all]: -13.08                       \n",
      "epsilon: 0.01, frames_total: 259210\n",
      "Elapsed time:  00:32:59\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: -6.30, [last 100]: -9.69, [all]: -12.60                       \n",
      "epsilon: 0.01, frames_total: 286294\n",
      "Elapsed time:  00:36:42\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: -9.70, [last 100]: -9.05, [all]: -12.40                       \n",
      "epsilon: 0.01, frames_total: 312810\n",
      "Elapsed time:  00:40:22\n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 0.00, [last 100]: -7.50, [all]: -11.63                       \n",
      "epsilon: 0.01, frames_total: 340377\n",
      "Elapsed time:  00:43:56\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: -2.60, [last 100]: -6.34, [all]: -11.11                       \n",
      "epsilon: 0.01, frames_total: 368551\n",
      "Elapsed time:  00:47:28\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 5.10, [last 100]: -4.52, [all]: -10.21                       \n",
      "epsilon: 0.01, frames_total: 391534\n",
      "Elapsed time:  00:50:30\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 4.80, [last 100]: -3.33, [all]: -9.42                       \n",
      "epsilon: 0.01, frames_total: 414738\n",
      "Elapsed time:  00:53:42\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: -1.10, [last 100]: -2.40, [all]: -9.01                       \n",
      "epsilon: 0.01, frames_total: 442767\n",
      "Elapsed time:  00:57:29\n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 8.00, [last 100]: -0.84, [all]: -8.20                       \n",
      "epsilon: 0.01, frames_total: 465394\n",
      "Elapsed time:  01:00:24\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 7.40, [last 100]: 0.15, [all]: -7.50                       \n",
      "epsilon: 0.01, frames_total: 490956\n",
      "Elapsed time:  01:03:43\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: -0.10, [last 100]: 0.55, [all]: -7.18                       \n",
      "epsilon: 0.01, frames_total: 520360\n",
      "Elapsed time:  01:07:30\n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 6.40, [last 100]: 1.82, [all]: -6.61                       \n",
      "epsilon: 0.01, frames_total: 546963\n",
      "Elapsed time:  01:11:00\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 10.10, [last 100]: 3.80, [all]: -5.95                       \n",
      "epsilon: 0.01, frames_total: 576615\n",
      "Elapsed time:  01:14:45\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 10.80, [last 100]: 4.88, [all]: -5.31                       \n",
      "epsilon: 0.01, frames_total: 603376\n",
      "Elapsed time:  01:18:07\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 10.50, [last 100]: 6.19, [all]: -4.72                       \n",
      "epsilon: 0.01, frames_total: 626291\n",
      "Elapsed time:  01:21:08\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 9.10, [last 100]: 6.59, [all]: -4.23                       \n",
      "epsilon: 0.01, frames_total: 653738\n",
      "Elapsed time:  01:24:51\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 9.30, [last 100]: 7.04, [all]: -3.77                       \n",
      "epsilon: 0.01, frames_total: 680740\n",
      "Elapsed time:  01:28:29\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 15.50, [last 100]: 8.70, [all]: -3.13                       \n",
      "epsilon: 0.01, frames_total: 703791\n",
      "Elapsed time:  01:31:45\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 13.70, [last 100]: 9.27, [all]: -2.59                       \n",
      "epsilon: 0.01, frames_total: 729611\n",
      "Elapsed time:  01:35:06\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 16.00, [last 100]: 10.13, [all]: -2.01                       \n",
      "epsilon: 0.01, frames_total: 752546\n",
      "Elapsed time:  01:38:02\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 16.20, [last 100]: 11.76, [all]: -1.46                       \n",
      "epsilon: 0.01, frames_total: 775142\n",
      "Elapsed time:  01:40:56\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 15.90, [last 100]: 12.71, [all]: -0.95                       \n",
      "epsilon: 0.01, frames_total: 797483\n",
      "Elapsed time:  01:43:50\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 15.30, [last 100]: 13.23, [all]: -0.48                       \n",
      "epsilon: 0.01, frames_total: 820243\n",
      "Elapsed time:  01:46:43\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 15.30, [last 100]: 13.68, [all]: -0.05                       \n",
      "epsilon: 0.01, frames_total: 843598\n",
      "Elapsed time:  01:49:46\n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 15.00, [last 100]: 14.13, [all]: 0.36                       \n",
      "epsilon: 0.01, frames_total: 866341\n",
      "Elapsed time:  01:52:45\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 14.90, [last 100]: 14.71, [all]: 0.74                       \n",
      "epsilon: 0.01, frames_total: 889887\n",
      "Elapsed time:  01:55:44\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 17.20, [last 100]: 15.50, [all]: 1.16                       \n",
      "epsilon: 0.01, frames_total: 911023\n",
      "Elapsed time:  01:58:41\n",
      "\n",
      "*** Episode 400 ***                       \n",
      "Av.reward: [last 10]: 18.60, [last 100]: 15.81, [all]: 1.60                       \n",
      "epsilon: 0.01, frames_total: 931846\n",
      "Elapsed time:  02:01:40\n",
      "\n",
      "*** Episode 410 ***                       \n",
      "Av.reward: [last 10]: 16.30, [last 100]: 16.07, [all]: 1.95                       \n",
      "epsilon: 0.01, frames_total: 952402\n",
      "Elapsed time:  02:04:31\n",
      "\n",
      "*** Episode 420 ***                       \n",
      "Av.reward: [last 10]: 15.60, [last 100]: 16.03, [all]: 2.28                       \n",
      "epsilon: 0.01, frames_total: 974223\n",
      "Elapsed time:  02:07:19\n",
      "\n",
      "*** Episode 430 ***                       \n",
      "Av.reward: [last 10]: 15.90, [last 100]: 16.00, [all]: 2.59                       \n",
      "epsilon: 0.01, frames_total: 996056\n",
      "Elapsed time:  02:10:24\n",
      "\n",
      "*** Episode 440 ***                       \n",
      "Av.reward: [last 10]: 17.00, [last 100]: 16.11, [all]: 2.92                       \n",
      "epsilon: 0.01, frames_total: 1017139\n",
      "Elapsed time:  02:13:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 450 ***                       \n",
      "Av.reward: [last 10]: 18.00, [last 100]: 16.38, [all]: 3.25                       \n",
      "epsilon: 0.01, frames_total: 1037575\n",
      "Elapsed time:  02:16:16\n",
      "\n",
      "*** Episode 460 ***                       \n",
      "Av.reward: [last 10]: 19.20, [last 100]: 16.77, [all]: 3.60                       \n",
      "epsilon: 0.01, frames_total: 1056564\n",
      "Elapsed time:  02:18:53\n",
      "\n",
      "*** Episode 470 ***                       \n",
      "Av.reward: [last 10]: 16.90, [last 100]: 16.96, [all]: 3.88                       \n",
      "epsilon: 0.01, frames_total: 1079342\n",
      "Elapsed time:  02:21:56\n",
      "\n",
      "*** Episode 480 ***                       \n",
      "Av.reward: [last 10]: 17.70, [last 100]: 17.24, [all]: 4.17                       \n",
      "epsilon: 0.01, frames_total: 1099332\n",
      "Elapsed time:  02:24:48\n",
      "\n",
      "*** Episode 490 ***                       \n",
      "Av.reward: [last 10]: 15.40, [last 100]: 17.06, [all]: 4.40                       \n",
      "epsilon: 0.01, frames_total: 1123235\n",
      "Elapsed time:  02:28:02\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average reward: 4.60\n",
      "Average reward (last 100 episodes): 16.75\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env)\n",
    "\n",
    "directory = './PongVideos/'\n",
    "env = gym.wrappers.Monitor(env, directory, video_callable=lambda episode_id: episode_id%20==0)\n",
    "\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.0001\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 512\n",
    "\n",
    "replay_mem_size = 100000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 2000\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 10000\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 18\n",
    "\n",
    "clip_error = True\n",
    "normalize_image = True\n",
    "\n",
    "file2save = 'pong_save.pth'\n",
    "save_model_frequency = 10000\n",
    "resume_previous_training = False\n",
    "\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon\n",
    "\n",
    "#This allows us to load a pretrained model\n",
    "def load_model():\n",
    "    return torch.load(file2save)\n",
    "\n",
    "#This allows us to save the model while training\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)\n",
    "    \n",
    "def preprocess_frame(frame):\n",
    "    frame = frame.transpose((2,0,1))\n",
    "    frame = torch.from_numpy(frame)\n",
    "    frame = frame.to(device, dtype=torch.float32)\n",
    "    frame = frame.unsqueeze(1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def plot_results():\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards_total, alpha=0.6, color='red')\n",
    "    plt.savefig(\"Pong-results.png\")\n",
    "    plt.close()\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.advantage1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer,1)\n",
    "\n",
    "        #self.activation = nn.Tanh()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if normalize_image:\n",
    "            x = x / 255\n",
    "        \n",
    "        output_conv = self.conv1(x)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv2(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv3(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        \n",
    "        output_conv = output_conv.view(output_conv.size(0), -1) # flatten\n",
    "        \n",
    "        output_advantage = self.advantage1(output_conv)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value1(output_conv)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "\n",
    "        return output_final\n",
    "    \n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.number_of_frames = 0\n",
    "        \n",
    "        #Allows us to load the prior model \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print(\"Loading saved model {}... \".format(file2save))\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = preprocess_frame(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                \n",
    "                action = torch.max(action_from_nn,1)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = [ preprocess_frame(frame) for frame in state ] \n",
    "        state = torch.cat(state)\n",
    "        \n",
    "        new_state = [ preprocess_frame(frame) for frame in new_state ] \n",
    "        new_state = torch.cat(new_state)\n",
    "\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "\n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        \n",
    "        \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.number_of_frames % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        if self.number_of_frames % save_model_frequency == 0:\n",
    "            save_model(self.nn)\n",
    "        \n",
    "        self.number_of_frames += 1\n",
    "        \n",
    "        #Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "rewards_total = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            rewards_total.append(score)\n",
    "            \n",
    "            mean_reward_100 = sum(rewards_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0 and i_episode > 0):\n",
    "                \n",
    "                plot_results()\n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(rewards_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(rewards_total)/len(rewards_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(rewards_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(rewards_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
