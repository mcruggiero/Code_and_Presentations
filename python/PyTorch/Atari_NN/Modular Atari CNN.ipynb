{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular Atari CNN\n",
    "<br><strong>\n",
    "Michael Ruggiero<br>\n",
    "michael@mcruggiero.com<br>\n",
    "Tuesday, August 13th, 2019<br></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Environment\" data-toc-modified-id=\"Environment-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Environment</a></span></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Functions-and-Classes\" data-toc-modified-id=\"Functions-and-Classes-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Functions and Classes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Report-Functions\" data-toc-modified-id=\"Report-Functions-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Report Functions</a></span></li><li><span><a href=\"#Image-Preprocessing-Functions\" data-toc-modified-id=\"Image-Preprocessing-Functions-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Image Preprocessing Functions</a></span></li><li><span><a href=\"#Exploration-vs-Exploitation\" data-toc-modified-id=\"Exploration-vs-Exploitation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Exploration vs Exploitation</a></span></li><li><span><a href=\"#Experience-Replay\" data-toc-modified-id=\"Experience-Replay-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Experience Replay</a></span></li></ul></li><li><span><a href=\"#Convolutional-Neural-Network\" data-toc-modified-id=\"Convolutional-Neural-Network-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Convolutional Neural Network</a></span></li><li><span><a href=\"#Q-Learning-Agent\" data-toc-modified-id=\"Q-Learning-Agent-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Q Learning Agent</a></span></li><li><span><a href=\"#Episode-Loop\" data-toc-modified-id=\"Episode-Loop-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Episode Loop</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:33.710919Z",
     "start_time": "2019-08-13T18:04:33.068040Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from time import time\n",
    "from time import sleep\n",
    "import gym\n",
    "import math\n",
    "\n",
    "import os.path\n",
    "\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:34.540732Z",
     "start_time": "2019-08-13T18:04:34.382921Z"
    }
   },
   "outputs": [],
   "source": [
    "#Check if gpu can be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Set tensor\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "#Set index tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "#Check out other Environments at:\n",
    "#https://github.com/openai/gym/wiki/Table-of-environments\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "#env = wrap_deepmind(env)\n",
    "\n",
    "#This makes a directory with videos where progress can be monitored\n",
    "directory = './CNN_Videos/'\n",
    "env = gym.wrappers.Monitor(env,\n",
    "                           directory,\n",
    "                           force = True, #To prevent overwrite remove\n",
    "                           video_callable=lambda episode_id: episode_id%20==0)\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:35.352026Z",
     "start_time": "2019-08-13T18:04:35.346304Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_episodes = 25\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 512\n",
    "\n",
    "replay_mem_size = 100000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 2000\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 10000\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 18\n",
    "\n",
    "clip_error = True\n",
    "normalize_image = True\n",
    "\n",
    "file2save = 'cnn_{}.pth'.format(env_id)\n",
    "save_model_frequency = 10000\n",
    "\n",
    "resume_previous_training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:36.637653Z",
     "start_time": "2019-08-13T18:04:36.633957Z"
    }
   },
   "outputs": [],
   "source": [
    "#This allows us to load a pretrained model\n",
    "def load_model():\n",
    "    return torch.load(file2save)\n",
    "\n",
    "#This allows us to save the model while training\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)\n",
    "\n",
    "def plot_results():\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards_total, alpha=0.6, color='red')\n",
    "    plt.savefig(\"Pong-results.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:37.710716Z",
     "start_time": "2019-08-13T18:04:37.707271Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \n",
    "    #This reworks the order of the states to make sure that pytorch can handle it\n",
    "    frame = frame.transpose((2,0,1))\n",
    "\n",
    "    #Change the array to pytorch tensor\n",
    "    frame = torch.from_numpy(frame)\n",
    "\n",
    "    #Feed the action to the device\n",
    "    frame = frame.to(device, dtype=torch.float32) #Calculations use float\n",
    "\n",
    "    #Need to feed into our network a 4d tensor, not 3d.  \n",
    "    frame = frame.unsqueeze(1)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:38.663511Z",
     "start_time": "2019-08-13T18:04:38.661020Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:12.762110Z",
     "start_time": "2019-08-13T18:04:12.755049Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:13.602311Z",
     "start_time": "2019-08-13T18:04:13.561484Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    #This is the architecture used by deepmind\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        #out_channels is the number of filters\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.advantage1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer,1)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if normalize_image:\n",
    "            x = x / 255\n",
    "        \n",
    "        output_conv = self.conv1(x)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv2(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv3(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "\n",
    "        #http://cs231n.github.io/convolutional-networks/\n",
    "        #View flattens out the network into a 7X7X64 layer tensor. \n",
    "        output_conv = output_conv.view(output_conv.size(0), -1)\n",
    "\n",
    "        output_advantage = self.advantage1(output_conv)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value1(output_conv)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        output_final = output_value+ output_advantage - output_advantage.mean()\n",
    "\n",
    "        return output_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:14.251112Z",
     "start_time": "2019-08-13T18:04:14.237812Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = CNN().to(device)\n",
    "        self.target_nn = CNN().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "\n",
    "        #This is a frame counter for the class\n",
    "        self.frame_number = 0\n",
    "        \n",
    "        #Allows us to load the prior model \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print(\"Loading saved model {}... \".format(file2save))\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = preprocess_frame(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                \n",
    "                action = torch.max(action_from_nn,1)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "\n",
    "        #This loops through all of the frames into a list\n",
    "        state = [ preprocess_frame(frame) for frame in state ] \n",
    "        state = torch.cat(state)\n",
    "\n",
    "        #Preprocess new_state\n",
    "        new_state = [preprocess_frame(frame) for frame in new_state] \n",
    "        new_state = torch.cat(new_state)\n",
    "\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        new_state_indexes = self.nn(new_state).detach()\n",
    "        max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "        \n",
    "        new_state_values = self.target_nn(new_state).detach()\n",
    "        max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "\n",
    "        #Updates the nn\n",
    "        if self.frae_number % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "\n",
    "        #Saves current model\n",
    "        if self.frame_number % save_model_frequency == 0:\n",
    "            save_model(self.nn)\n",
    "        \n",
    "        self.frame_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-13T18:04:15.129Z"
    }
   },
   "outputs": [],
   "source": [
    "memory = ExperienceReplay(replay_mem_size)\n",
    "agent = Agent()\n",
    "\n",
    "rewards_total = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "        # print(\"Image\")\n",
    "        # print(state)\n",
    "        # print(type(state))\n",
    "        # print(state.dtype)\n",
    "        # print(state.shape)\n",
    "         \n",
    "        if done:\n",
    "            rewards_total.append(score)\n",
    "            \n",
    "            mean_reward_100 = sum(rewards_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0 and i_episode > 0):\n",
    "                \n",
    "                plot_results()\n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(rewards_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(rewards_total)/len(rewards_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(rewards_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(rewards_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "\n",
    "\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:04:21.699672Z",
     "start_time": "2019-08-13T18:04:21.644451Z"
    }
   },
   "outputs": [],
   "source": [
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "rewards_total = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            rewards_total.append(score)\n",
    "            \n",
    "            mean_reward_100 = sum(rewards_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0 and i_episode > 0):\n",
    "                \n",
    "                plot_results()\n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(rewards_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(rewards_total)/len(rewards_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(rewards_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(rewards_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "768px",
    "left": "26px",
    "top": "110px",
    "width": "185px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
