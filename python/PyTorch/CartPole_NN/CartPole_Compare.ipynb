{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-Imports\" data-toc-modified-id=\"Library-Imports-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Library Imports</a></span></li><li><span><a href=\"#Cuda-Support\" data-toc-modified-id=\"Cuda-Support-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Cuda Support</a></span></li></ul></li><li><span><a href=\"#CartPole-v1-Environment\" data-toc-modified-id=\"CartPole-v1-Environment-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>CartPole v1 Environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Action/Input-Space\" data-toc-modified-id=\"Action/Input-Space-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Action/Input Space</a></span></li><li><span><a href=\"#Observation/Output-Space\" data-toc-modified-id=\"Observation/Output-Space-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Observation/Output Space</a></span></li></ul></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Informational-Functions\" data-toc-modified-id=\"Informational-Functions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Informational Functions</a></span></li><li><span><a href=\"#Agent-Factory\" data-toc-modified-id=\"Agent-Factory-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Agent Factory</a></span></li><li><span><a href=\"#Simulation-Function\" data-toc-modified-id=\"Simulation-Function-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Simulation Function</a></span></li></ul></li><li><span><a href=\"#Random-Agent\" data-toc-modified-id=\"Random-Agent-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Random Agent</a></span></li><li><span><a href=\"#One-Layer-Neural-Nets\" data-toc-modified-id=\"One-Layer-Neural-Nets-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>One Layer Neural Nets</a></span><ul class=\"toc-item\"><li><span><a href=\"#BNN-Neural-Net-Definition\" data-toc-modified-id=\"BNN-Neural-Net-Definition-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>BNN Neural Net Definition</a></span></li><li><span><a href=\"#BNN_1-Parameters\" data-toc-modified-id=\"BNN_1-Parameters-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>BNN_1 Parameters</a></span></li><li><span><a href=\"#BNN_1-Class\" data-toc-modified-id=\"BNN_1-Class-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>BNN_1 Class</a></span></li><li><span><a href=\"#BNN_1-simulation\" data-toc-modified-id=\"BNN_1-simulation-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>BNN_1 simulation</a></span></li><li><span><a href=\"#BNN_2-Parameters\" data-toc-modified-id=\"BNN_2-Parameters-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>BNN_2 Parameters</a></span></li><li><span><a href=\"#BNN_2-Definition\" data-toc-modified-id=\"BNN_2-Definition-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>BNN_2 Definition</a></span></li><li><span><a href=\"#BNN_2-simulation\" data-toc-modified-id=\"BNN_2-simulation-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>BNN_2 simulation</a></span></li></ul></li><li><span><a href=\"#Two-Layer-Neural-Nets\" data-toc-modified-id=\"Two-Layer-Neural-Nets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Two Layer Neural Nets</a></span><ul class=\"toc-item\"><li><span><a href=\"#TLN-With-Tanh,-Neural-Net-Definition\" data-toc-modified-id=\"TLN-With-Tanh,-Neural-Net-Definition-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>TLN With Tanh, Neural Net Definition</a></span></li><li><span><a href=\"#TLN_1-Parameters\" data-toc-modified-id=\"TLN_1-Parameters-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>TLN_1 Parameters</a></span></li><li><span><a href=\"#TLN_1-Simulation\" data-toc-modified-id=\"TLN_1-Simulation-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>TLN_1 Simulation</a></span></li></ul></li><li><span><a href=\"#Random-Cartpole\" data-toc-modified-id=\"Random-Cartpole-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Random Cartpole</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:05.191436Z",
     "start_time": "2019-08-02T17:29:04.750757Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from time import time\n",
    "import gym\n",
    "import math\n",
    "\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:05.814129Z",
     "start_time": "2019-08-02T17:29:05.800604Z"
    }
   },
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Tensor = torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole v1 Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:06.716649Z",
     "start_time": "2019-08-02T17:29:06.684726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Space:\t4\n",
      "Output Space:\t2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "inputs = env.observation_space.shape[0]\n",
    "outputs = env.action_space.n\n",
    "\n",
    "print(\"Input Space:\\t{}\\nOutput Space:\\t{}\".format(inputs, outputs))\n",
    "score_to_solve = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action/Input Space\n",
    "\n",
    "|Num|Action|\n",
    "|-|-|\n",
    "|0|Push cart to the left|\n",
    "|1|Push cart to the right|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation/Output Space     \n",
    "\n",
    "\n",
    "|Num|Observation|Min|Max|\n",
    "|-|---------------------|-|-|\n",
    "|0|Cart Position|-4.8|4.8|\n",
    "|1|Cart Velocity|-Inf|Inf|\n",
    "|2|Pole Angle|-24 deg|24 deg|\n",
    "|3|Pole Velocity At Tip|-Inf|Inf|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informational Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:08.805179Z",
     "start_time": "2019-08-02T17:29:08.796560Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_information(steps_total, shadow_run):\n",
    "    e_list = []\n",
    "    for i in shadow_run:\n",
    "        for j in shadow_run[i][\"epsilon\"]:\n",
    "            e_list.append(j)\n",
    "    \n",
    "    #s = list(range(sum(steps_total)))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,10), nrows=2, ncols=1)\n",
    "    fig.subplots_adjust(hspace=.25)\n",
    "\n",
    "    ax[0].set(title=\"Rewards\",\n",
    "              xlabel='Session', \n",
    "              ylabel='Steps')\n",
    "    ax[0].bar(torch.arange(len(steps_total)), \n",
    "            steps_total, \n",
    "            alpha=0.6, \n",
    "            color='green',\n",
    "            width= 1)\n",
    "    \n",
    "    ax[1].set(title=\"Epsilon\",\n",
    "          xlabel='Steps', \n",
    "          ylabel='Session')\n",
    "    ax[1].plot(e_list)\n",
    "    plt.show()\n",
    "    \n",
    "def report(report_interval,\n",
    "           episode,\n",
    "           start_time,\n",
    "           steps_total,\n",
    "           epsilon,\n",
    "           frames_total,\n",
    "           solved):\n",
    "    \n",
    "    if episode % report_interval == 0 and episode != 0:\n",
    "        print(\"\"\"\\t\\t\\t *** Episode {} at Total Seconds {:2f}***\n",
    "                 Average Reward for last {} steps: {:2f}\n",
    "                 Average Reward for all steps: {:2f}\n",
    "                 Epsilon: {}, Frames Total :{}\n",
    "                 Solved {} Times\n",
    "              \"\"\".format(episode,\n",
    "                   (time() - start_time),\n",
    "                   str(report_interval),\n",
    "                   sum(steps_total[-report_interval:])/report_interval,\n",
    "                   sum(steps_total)/len(steps_total),\n",
    "                   epsilon,\n",
    "                   frames_total,\n",
    "                   solved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:09.740315Z",
     "start_time": "2019-08-02T17:29:09.735363Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "  \n",
    "    def factory(type):\n",
    "        if type == \"Random\":\n",
    "            return Random_Agent(type)\n",
    "        elif type == \"BNN_1\":\n",
    "            return BNN_1(type)\n",
    "        elif type == \"BNN_2\":\n",
    "            return BNN_2(type)\n",
    "        elif type == \"TLN_1\":\n",
    "            return TLN_1(type)\n",
    "        \n",
    "    factory = staticmethod(factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:10.732130Z",
     "start_time": "2019-08-02T17:29:10.719570Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulation(num_episodes, agent, record, report_interval, extra_name  = \"\"):\n",
    "    #Simulation Initialization\n",
    "    actor = Agent.factory(agent)\n",
    "    start_time = time()\n",
    "    solved, frames_total = 0, 0\n",
    "    steps_total = []\n",
    "    \n",
    "    if extra_name != \"\": extra_name = \"_\" + extra_name\n",
    "    \n",
    "    #The recorded dictionary for the simulation\n",
    "    shadow_run = {}\n",
    "    \n",
    "    #Loop through different episodes\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        # Episode Initializations\n",
    "        state = env.reset()\n",
    "        epi_name = actor.name + extra_name + \"_\"+ str(episode).zfill(4)\n",
    "        shadow_run[epi_name] = {} \n",
    "        \n",
    "        #A facny way to make lists\n",
    "        blanks = [-1 for i in range(500)]\n",
    "        \n",
    "        #Input for episode\n",
    "        actions, steps = [blanks for i in range(2)] \n",
    "        \n",
    "        #Outputs for episodes\n",
    "        cart_position, cart_velocity = [blanks for i in range(2)]\n",
    "        pole_angle, pole_velocity  = [blanks for i in range(2)]\n",
    "        \n",
    "        step = 0\n",
    "                \n",
    "        while True:\n",
    "            \n",
    "            #Calculate Learning Rate\n",
    "            epsilon = actor.calculate_epsilon(frames_total)\n",
    "\n",
    "            #Actor acts\n",
    "            action = actor.select_action(state, epsilon)\n",
    "\n",
    "            #Record States\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            #Optimize\n",
    "            actor.optimize(state, action, new_state, reward, done)\n",
    "\n",
    "            #Store Inputs for step\n",
    "            actions[step] = action\n",
    "            \n",
    "            ###\n",
    "            #**DO THIS**\n",
    "            #On update, make a class called Information to clean syntax\n",
    "            ###\n",
    "            \n",
    "            #Unpack Output for step\n",
    "            cart_position[step] = new_state[0]\n",
    "            cart_velocity[step] = new_state[1]\n",
    "            pole_angle[step] = new_state[2]\n",
    "            pole_velocity[step] = new_state[3]         \n",
    "\n",
    "            if done:\n",
    "                steps_total.append(step)\n",
    "                \n",
    "                ###\n",
    "                #**DO THIS**\n",
    "                #On update, clean up the following syntax\n",
    "                ###\n",
    "                \n",
    "                #Below is an overpacked syntax to add a column for each part\n",
    "                #.zfill pads zeros in the front of the string\n",
    "                \n",
    "                for i in range(500):\n",
    "                    D = shadow_run[epi_name]\n",
    "                    D[\"action_{}\".format(str(i).zfill(3))]        = actions[i]\n",
    "                    D[\"cart_position_{}\".format(str(i).zfill(3))] = cart_position[i]\n",
    "                    D[\"cart_velocity_{}\".format(str(i).zfill(3))] = cart_velocity[i]\n",
    "                    D[\"pole_angle_{}\".format(str(i).zfill(3))]    = pole_angle[i]\n",
    "                    D[\"pole_velocity_{}\".format(str(i).zfill(3))] = pole_velocity[i]                                     \n",
    "                \n",
    "                shadow_run[epi_name] = D\n",
    "                #Not needed for this environment\n",
    "                #shadow_run[episode][\"information\"] = information\n",
    "\n",
    "                if step >= score_to_solve: solved += 1\n",
    "\n",
    "                report(report_interval, episode, start_time, steps_total, epsilon, frames_total, solved)\n",
    "\n",
    "                break\n",
    "\n",
    "            step += 1\n",
    "            frames_total += 1\n",
    "            state = new_state\n",
    "        \n",
    "    #Clean up after Episodes\n",
    "    record[actor.name + extra_name] = shadow_run    \n",
    "    env.close()\n",
    "    env.env.close()\n",
    "\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:11.858313Z",
     "start_time": "2019-08-02T17:29:11.854414Z"
    }
   },
   "outputs": [],
   "source": [
    "class Random_Agent(Agent):\n",
    "    def __init__(self, Agent):\n",
    "        self.name = Agent\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        action = env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def calculate_epsilon(self,steps_done):\n",
    "        return 0    \n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:13.669966Z",
     "start_time": "2019-08-02T17:29:12.431386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 0.142594***\n",
      "                 Average Reward for last 100 steps: 22.740000\n",
      "                 Average Reward for all steps: 22.811881\n",
      "                 Epsilon: 0, Frames Total :2304\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 0.260123***\n",
      "                 Average Reward for last 100 steps: 19.680000\n",
      "                 Average Reward for all steps: 21.253731\n",
      "                 Epsilon: 0, Frames Total :4272\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 0.381009***\n",
      "                 Average Reward for last 100 steps: 23.630000\n",
      "                 Average Reward for all steps: 22.043189\n",
      "                 Epsilon: 0, Frames Total :6635\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 0.501348***\n",
      "                 Average Reward for last 100 steps: 20.490000\n",
      "                 Average Reward for all steps: 21.655860\n",
      "                 Epsilon: 0, Frames Total :8684\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 0.621519***\n",
      "                 Average Reward for last 100 steps: 20.750000\n",
      "                 Average Reward for all steps: 21.475050\n",
      "                 Epsilon: 0, Frames Total :10759\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 0.741763***\n",
      "                 Average Reward for last 100 steps: 20.990000\n",
      "                 Average Reward for all steps: 21.394343\n",
      "                 Epsilon: 0, Frames Total :12858\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 0.863776***\n",
      "                 Average Reward for last 100 steps: 22.940000\n",
      "                 Average Reward for all steps: 21.614836\n",
      "                 Epsilon: 0, Frames Total :15152\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 0.986022***\n",
      "                 Average Reward for last 100 steps: 23.410000\n",
      "                 Average Reward for all steps: 21.838951\n",
      "                 Epsilon: 0, Frames Total :17493\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 1.110435***\n",
      "                 Average Reward for last 100 steps: 21.400000\n",
      "                 Average Reward for all steps: 21.790233\n",
      "                 Epsilon: 0, Frames Total :19633\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"Random\", {}, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:15.799970Z",
     "start_time": "2019-08-02T17:29:15.796446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(record[\"Random\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:20.116274Z",
     "start_time": "2019-08-02T17:29:19.578438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['action_000', 'action_001', 'action_002', 'action_003', 'action_004',\n",
       "       'action_005', 'action_006', 'action_007', 'action_008', 'action_009',\n",
       "       ...\n",
       "       'pole_velocity_490', 'pole_velocity_491', 'pole_velocity_492',\n",
       "       'pole_velocity_493', 'pole_velocity_494', 'pole_velocity_495',\n",
       "       'pole_velocity_496', 'pole_velocity_497', 'pole_velocity_498',\n",
       "       'pole_velocity_499'],\n",
       "      dtype='object', length=2500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(record[\"Random\"]).T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:20.673989Z",
     "start_time": "2019-08-02T17:29:20.117483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_000</th>\n",
       "      <th>action_001</th>\n",
       "      <th>action_002</th>\n",
       "      <th>action_003</th>\n",
       "      <th>action_004</th>\n",
       "      <th>action_005</th>\n",
       "      <th>action_006</th>\n",
       "      <th>action_007</th>\n",
       "      <th>action_008</th>\n",
       "      <th>action_009</th>\n",
       "      <th>...</th>\n",
       "      <th>pole_velocity_490</th>\n",
       "      <th>pole_velocity_491</th>\n",
       "      <th>pole_velocity_492</th>\n",
       "      <th>pole_velocity_493</th>\n",
       "      <th>pole_velocity_494</th>\n",
       "      <th>pole_velocity_495</th>\n",
       "      <th>pole_velocity_496</th>\n",
       "      <th>pole_velocity_497</th>\n",
       "      <th>pole_velocity_498</th>\n",
       "      <th>pole_velocity_499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random_0000</th>\n",
       "      <td>-0.310608</td>\n",
       "      <td>-0.023597</td>\n",
       "      <td>0.261409</td>\n",
       "      <td>-0.038900</td>\n",
       "      <td>-0.337600</td>\n",
       "      <td>-0.051307</td>\n",
       "      <td>0.232801</td>\n",
       "      <td>-0.068528</td>\n",
       "      <td>0.216755</td>\n",
       "      <td>-0.083568</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_0001</th>\n",
       "      <td>0.302538</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>-0.292654</td>\n",
       "      <td>-0.589287</td>\n",
       "      <td>-0.302476</td>\n",
       "      <td>-0.604543</td>\n",
       "      <td>-0.323510</td>\n",
       "      <td>-0.046471</td>\n",
       "      <td>-0.355945</td>\n",
       "      <td>-0.081362</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_0002</th>\n",
       "      <td>-0.274389</td>\n",
       "      <td>0.021212</td>\n",
       "      <td>-0.270266</td>\n",
       "      <td>0.023749</td>\n",
       "      <td>0.316062</td>\n",
       "      <td>0.608525</td>\n",
       "      <td>0.317619</td>\n",
       "      <td>0.615861</td>\n",
       "      <td>0.330880</td>\n",
       "      <td>0.049916</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_0003</th>\n",
       "      <td>-0.325925</td>\n",
       "      <td>-0.043888</td>\n",
       "      <td>-0.348827</td>\n",
       "      <td>-0.069210</td>\n",
       "      <td>-0.376509</td>\n",
       "      <td>-0.099649</td>\n",
       "      <td>0.174698</td>\n",
       "      <td>0.448381</td>\n",
       "      <td>0.723243</td>\n",
       "      <td>1.001101</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_0004</th>\n",
       "      <td>0.314862</td>\n",
       "      <td>0.019873</td>\n",
       "      <td>-0.273140</td>\n",
       "      <td>0.019338</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>0.600959</td>\n",
       "      <td>0.893792</td>\n",
       "      <td>0.605063</td>\n",
       "      <td>0.907156</td>\n",
       "      <td>1.212880</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             action_000  action_001  action_002  action_003  action_004  \\\n",
       "Random_0000   -0.310608   -0.023597    0.261409   -0.038900   -0.337600   \n",
       "Random_0001    0.302538    0.004005   -0.292654   -0.589287   -0.302476   \n",
       "Random_0002   -0.274389    0.021212   -0.270266    0.023749    0.316062   \n",
       "Random_0003   -0.325925   -0.043888   -0.348827   -0.069210   -0.376509   \n",
       "Random_0004    0.314862    0.019873   -0.273140    0.019338    0.310087   \n",
       "\n",
       "             action_005  action_006  action_007  action_008  action_009  ...  \\\n",
       "Random_0000   -0.051307    0.232801   -0.068528    0.216755   -0.083568  ...   \n",
       "Random_0001   -0.604543   -0.323510   -0.046471   -0.355945   -0.081362  ...   \n",
       "Random_0002    0.608525    0.317619    0.615861    0.330880    0.049916  ...   \n",
       "Random_0003   -0.099649    0.174698    0.448381    0.723243    1.001101  ...   \n",
       "Random_0004    0.600959    0.893792    0.605063    0.907156    1.212880  ...   \n",
       "\n",
       "             pole_velocity_490  pole_velocity_491  pole_velocity_492  \\\n",
       "Random_0000               -1.0               -1.0               -1.0   \n",
       "Random_0001               -1.0               -1.0               -1.0   \n",
       "Random_0002               -1.0               -1.0               -1.0   \n",
       "Random_0003               -1.0               -1.0               -1.0   \n",
       "Random_0004               -1.0               -1.0               -1.0   \n",
       "\n",
       "             pole_velocity_493  pole_velocity_494  pole_velocity_495  \\\n",
       "Random_0000               -1.0               -1.0               -1.0   \n",
       "Random_0001               -1.0               -1.0               -1.0   \n",
       "Random_0002               -1.0               -1.0               -1.0   \n",
       "Random_0003               -1.0               -1.0               -1.0   \n",
       "Random_0004               -1.0               -1.0               -1.0   \n",
       "\n",
       "             pole_velocity_496  pole_velocity_497  pole_velocity_498  \\\n",
       "Random_0000               -1.0               -1.0               -1.0   \n",
       "Random_0001               -1.0               -1.0               -1.0   \n",
       "Random_0002               -1.0               -1.0               -1.0   \n",
       "Random_0003               -1.0               -1.0               -1.0   \n",
       "Random_0004               -1.0               -1.0               -1.0   \n",
       "\n",
       "             pole_velocity_499  \n",
       "Random_0000               -1.0  \n",
       "Random_0001               -1.0  \n",
       "Random_0002               -1.0  \n",
       "Random_0003               -1.0  \n",
       "Random_0004               -1.0  \n",
       "\n",
       "[5 rows x 2500 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(record[\"Random\"]).T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Layer Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN Neural Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:22.295115Z",
     "start_time": "2019-08-02T17:29:22.292033Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork_1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,number_of_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:23.083940Z",
     "start_time": "2019-08-02T17:29:23.069872Z"
    }
   },
   "outputs": [],
   "source": [
    "###### PARAMS ######\n",
    "learning_rate = 0.025\n",
    "num_episodes = 1000\n",
    "gamma = 0.85\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.025\n",
    "egreedy_decay = 500\n",
    "\n",
    "####################\n",
    "\n",
    "#These lines are useful if I decide to change environments\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_1 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:24.066173Z",
     "start_time": "2019-08-02T17:29:24.059257Z"
    }
   },
   "outputs": [],
   "source": [
    "class BNN_1(object):\n",
    "    \n",
    "    def __init__(self,Agent):\n",
    "        self.nn = NeuralNetwork_1().to(device)\n",
    "        self.name = Agent\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_1 simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:32.652319Z",
     "start_time": "2019-08-02T17:29:25.120971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 0.596590***\n",
      "                 Average Reward for last 100 steps: 12.010000\n",
      "                 Average Reward for all steps: 11.990099\n",
      "                 Epsilon: 0.10265095795554963, Frames Total :1211\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 1.175857***\n",
      "                 Average Reward for last 100 steps: 11.830000\n",
      "                 Average Reward for all steps: 11.910448\n",
      "                 Epsilon: 0.03228796156610423, Frames Total :2394\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 1.627053***\n",
      "                 Average Reward for last 100 steps: 8.790000\n",
      "                 Average Reward for all steps: 10.873754\n",
      "                 Epsilon: 0.026256366577283762, Frames Total :3273\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 2.276345***\n",
      "                 Average Reward for last 100 steps: 15.030000\n",
      "                 Average Reward for all steps: 11.910224\n",
      "                 Epsilon: 0.02506217662749236, Frames Total :4776\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 3.145489***\n",
      "                 Average Reward for last 100 steps: 21.120000\n",
      "                 Average Reward for all steps: 13.748503\n",
      "                 Epsilon: 0.025000910263797006, Frames Total :6888\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 4.314307***\n",
      "                 Average Reward for last 100 steps: 31.620000\n",
      "                 Average Reward for all steps: 16.722130\n",
      "                 Epsilon: 0.025000001631882807, Frames Total :10050\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 4.845088***\n",
      "                 Average Reward for last 100 steps: 11.320000\n",
      "                 Average Reward for all steps: 15.951498\n",
      "                 Epsilon: 0.02500000016960797, Frames Total :11182\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 5.824376***\n",
      "                 Average Reward for last 100 steps: 26.170000\n",
      "                 Average Reward for all steps: 17.227216\n",
      "                 Epsilon: 0.02500000000090438, Frames Total :13799\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 6.607247***\n",
      "                 Average Reward for last 100 steps: 19.670000\n",
      "                 Average Reward for all steps: 17.498335\n",
      "                 Epsilon: 0.025000000000017696, Frames Total :15766\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"BNN_1\", record, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:32.656578Z",
     "start_time": "2019-08-02T17:29:32.653692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Random', 'BNN_1'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:32.668178Z",
     "start_time": "2019-08-02T17:29:32.658062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(record[\"BNN_1\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_2 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:32.672973Z",
     "start_time": "2019-08-02T17:29:32.669530Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_episodes = 1000\n",
    "gamma = 0.85\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 750\n",
    "\n",
    "#These lines are useful if I decide to change environments\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_2 Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:32.681711Z",
     "start_time": "2019-08-02T17:29:32.674113Z"
    }
   },
   "outputs": [],
   "source": [
    "#I might want to tinker more with these settings later, so I am copying it \n",
    "#From BNN_1 for now.\n",
    "\n",
    "###\n",
    "#**DO THIS**\n",
    "#Next time consider either significantly changing code\n",
    "#Or, simply duplicate the BNN_1 class\n",
    "###\n",
    "\n",
    "class BNN_2(object):\n",
    "    \n",
    "    def __init__(self,Agent):\n",
    "        self.nn = NeuralNetwork_1().to(device)\n",
    "        self.name = Agent\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_2 simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:37.085973Z",
     "start_time": "2019-08-02T17:29:32.682858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 0.542281***\n",
      "                 Average Reward for last 100 steps: 11.860000\n",
      "                 Average Reward for all steps: 11.861386\n",
      "                 Epsilon: 0.19016770820988002, Frames Total :1198\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 0.965622***\n",
      "                 Average Reward for last 100 steps: 8.610000\n",
      "                 Average Reward for all steps: 10.243781\n",
      "                 Epsilon: 0.06716193068769614, Frames Total :2059\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 1.394936***\n",
      "                 Average Reward for last 100 steps: 8.650000\n",
      "                 Average Reward for all steps: 9.714286\n",
      "                 Epsilon: 0.02803933760906884, Frames Total :2924\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 1.802954***\n",
      "                 Average Reward for last 100 steps: 8.450000\n",
      "                 Average Reward for all steps: 9.399002\n",
      "                 Epsilon: 0.01584676273471954, Frames Total :3769\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 2.244854***\n",
      "                 Average Reward for last 100 steps: 8.460000\n",
      "                 Average Reward for all steps: 9.211577\n",
      "                 Epsilon: 0.011892479983253315, Frames Total :4615\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 2.714493***\n",
      "                 Average Reward for last 100 steps: 8.450000\n",
      "                 Average Reward for all steps: 9.084859\n",
      "                 Epsilon: 0.010613375151686586, Frames Total :5460\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 3.132363***\n",
      "                 Average Reward for last 100 steps: 8.370000\n",
      "                 Average Reward for all steps: 8.982882\n",
      "                 Epsilon: 0.010200934049385477, Frames Total :6297\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 3.558524***\n",
      "                 Average Reward for last 100 steps: 8.390000\n",
      "                 Average Reward for all steps: 8.908864\n",
      "                 Epsilon: 0.010065648192962197, Frames Total :7136\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 3.987379***\n",
      "                 Average Reward for last 100 steps: 8.420000\n",
      "                 Average Reward for all steps: 8.854606\n",
      "                 Epsilon: 0.010021362635864508, Frames Total :7978\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"BNN_2\", record, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:29:37.089557Z",
     "start_time": "2019-08-02T17:29:37.087049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Random', 'BNN_1', 'BNN_2'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN With Tanh, Neural Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:43:16.715811Z",
     "start_time": "2019-08-02T17:43:16.707396Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_2, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer,number_of_outputs)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN_1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:43:17.400844Z",
     "start_time": "2019-08-02T17:43:17.385387Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:43:17.943505Z",
     "start_time": "2019-08-02T17:43:17.918563Z"
    }
   },
   "outputs": [],
   "source": [
    "class TLN_1(object):\n",
    "    \n",
    "    def __init__(self,Agent):\n",
    "        self.nn = NeuralNetwork_2().to(device)\n",
    "        self.name = Agent\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN_1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T17:45:57.538947Z",
     "start_time": "2019-08-02T17:43:19.103679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 5.835294***\n",
      "                 Average Reward for last 100 steps: 129.040000\n",
      "                 Average Reward for all steps: 127.920792\n",
      "                 Epsilon: 0.020000000005276103, Frames Total :12920\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 17.452231***\n",
      "                 Average Reward for last 100 steps: 259.400000\n",
      "                 Average Reward for all steps: 193.333333\n",
      "                 Epsilon: 0.02, Frames Total :38860\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 31.296522***\n",
      "                 Average Reward for last 100 steps: 281.410000\n",
      "                 Average Reward for all steps: 222.594684\n",
      "                 Epsilon: 0.02, Frames Total :67001\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 45.813516***\n",
      "                 Average Reward for last 100 steps: 270.710000\n",
      "                 Average Reward for all steps: 234.593516\n",
      "                 Epsilon: 0.02, Frames Total :94072\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 64.847548***\n",
      "                 Average Reward for last 100 steps: 356.660000\n",
      "                 Average Reward for all steps: 258.958084\n",
      "                 Epsilon: 0.02, Frames Total :129738\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 77.703475***\n",
      "                 Average Reward for last 100 steps: 237.060000\n",
      "                 Average Reward for all steps: 255.314476\n",
      "                 Epsilon: 0.02, Frames Total :153444\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 98.536765***\n",
      "                 Average Reward for last 100 steps: 386.630000\n",
      "                 Average Reward for all steps: 274.047076\n",
      "                 Epsilon: 0.02, Frames Total :192107\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 120.127730***\n",
      "                 Average Reward for last 100 steps: 398.800000\n",
      "                 Average Reward for all steps: 289.621723\n",
      "                 Epsilon: 0.02, Frames Total :231987\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 142.195533***\n",
      "                 Average Reward for last 100 steps: 409.570000\n",
      "                 Average Reward for all steps: 302.934517\n",
      "                 Epsilon: 0.02, Frames Total :272944\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"TLN_1\", record, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "gamma = 0.99\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.001\n",
    "egreedy_decay = num_episodes * 5\n",
    "report_interval = 10\n",
    "\n",
    "hidden_layer = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_agent = QNet_Agent()\n",
    "start_time = time()\n",
    "solved, frames_total = 0, 0\n",
    "steps_total = []\n",
    "shadow_run = {}\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Episode Initializations\n",
    "    state = env.reset()\n",
    "    shadow_run[episode] = {}\n",
    "    actions, states, shots = [], [], []\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "\n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        rendered = env.render(mode = 'rgb_array')\n",
    "\n",
    "        # Agent Optimize\n",
    "        qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "\n",
    "        # Store states for step\n",
    "        actions.append(action)\n",
    "        states.append(new_state)\n",
    "        shots.append(rendered)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            shadow_run[episode][\"actions\"] = actions\n",
    "            shadow_run[episode][\"states\"] = states\n",
    "            shadow_run[episode][\"shots\"] = shots\n",
    "            shadow_run[episode][\"steps\"] = step\n",
    "\n",
    "            if step >= score_to_solve: solved += 1\n",
    "\n",
    "            report(report_interval, episode, start_time, steps_total, epsilon, frames_total, solved)\n",
    "\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n",
    "\n",
    "plot_information(steps_total,shadow_run)\n",
    "display_frames(num_episodes-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T12:00:24.742649Z",
     "start_time": "2019-08-02T12:00:24.736764Z"
    }
   },
   "outputs": [],
   "source": [
    "class Car(object):\n",
    "  \n",
    "    def factory(type):\n",
    "        if type == \"Racecar\":\n",
    "            return Racecar(type)\n",
    "        if type == \"Van\":\n",
    "            return Van(type)\n",
    "\n",
    "    factory = staticmethod(factory)\n",
    "\n",
    "class Racecar(Car):\n",
    "    \n",
    "    def __init__(self,Car):\n",
    "        self.name = Car\n",
    "        \n",
    "    def drive(self):\n",
    "        print(\"Racecar driving.\")\n",
    "\n",
    "class Van(Car):\n",
    "    def drive(self):\n",
    "        print(\"Van driving.\")\n",
    "\n",
    "# Create object using factory.\n",
    "obj = Car.factory(\"Racecar\")\n",
    "obj.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object,name):\n",
    "    def __init__(self,name):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        if self.name == \"random\":\n",
    "        else:\n",
    "            self.nn = NeuralNetwork().to(device)\n",
    "            self.loss_func = nn.MSELoss()\n",
    "            #self.loss_func = nn.SmoothL1Loss()\n",
    "            self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "            #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T09:44:59.822975Z",
     "start_time": "2019-08-02T09:44:58.353174Z"
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "steps_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()    \n",
    "    step = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        #Random Step\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            steps_total.append(step)\n",
    "            print(\"Episode finished after %i steps\" % step )\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green')\n",
    "plt.show()\n",
    "\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T12:35:02.044747Z",
     "start_time": "2019-08-02T12:35:01.770782Z"
    }
   },
   "outputs": [],
   "source": [
    "test[\"Random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:00:22.875011Z",
     "start_time": "2019-08-02T13:00:22.871560Z"
    }
   },
   "outputs": [],
   "source": [
    "blanks = [-200 for i in range(500)]\n",
    "actions, states, information = [blanks for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:00:25.183021Z",
     "start_time": "2019-08-02T13:00:25.175250Z"
    }
   },
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:09:15.718114Z",
     "start_time": "2019-08-02T13:09:15.712287Z"
    }
   },
   "outputs": [],
   "source": [
    "bob = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:09:23.871441Z",
     "start_time": "2019-08-02T13:09:23.865264Z"
    }
   },
   "outputs": [],
   "source": [
    "bob[2] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:09:25.363473Z",
     "start_time": "2019-08-02T13:09:25.358178Z"
    }
   },
   "outputs": [],
   "source": [
    "bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T15:23:29.940524Z",
     "start_time": "2019-08-02T15:23:29.934723Z"
    }
   },
   "outputs": [],
   "source": [
    "str(54).zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
