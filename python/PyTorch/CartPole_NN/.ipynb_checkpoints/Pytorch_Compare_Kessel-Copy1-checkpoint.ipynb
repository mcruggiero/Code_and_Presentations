{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of DQL Networks\n",
    "<strong>Michael Ruggiero<br>\n",
    "Tuesday, August 6th, 2019<br>\n",
    "michael@mcruggiero.com</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Sources\" data-toc-modified-id=\"Sources-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Sources</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-Imports\" data-toc-modified-id=\"Library-Imports-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Library Imports</a></span></li><li><span><a href=\"#Cuda-Support\" data-toc-modified-id=\"Cuda-Support-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Cuda Support</a></span></li></ul></li><li><span><a href=\"#CartPole-v1-Environment\" data-toc-modified-id=\"CartPole-v1-Environment-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>CartPole v1 Environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Human-Demonstration\" data-toc-modified-id=\"Human-Demonstration-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Human Demonstration</a></span></li><li><span><a href=\"#Action/Input-Space\" data-toc-modified-id=\"Action/Input-Space-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Action/Input Space</a></span></li><li><span><a href=\"#Observation/Output-Space\" data-toc-modified-id=\"Observation/Output-Space-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Observation/Output Space</a></span></li></ul></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Report-Function\" data-toc-modified-id=\"Report-Function-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Report Function</a></span></li><li><span><a href=\"#Agent-Factory\" data-toc-modified-id=\"Agent-Factory-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Agent Factory</a></span></li><li><span><a href=\"#Simulation-Function\" data-toc-modified-id=\"Simulation-Function-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Simulation Function</a></span></li></ul></li><li><span><a href=\"#Random-Agent\" data-toc-modified-id=\"Random-Agent-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Random Agent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random_Agent-Class\" data-toc-modified-id=\"Random_Agent-Class-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Random_Agent Class</a></span></li><li><span><a href=\"#Random-Demonstration\" data-toc-modified-id=\"Random-Demonstration-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Random Demonstration</a></span></li><li><span><a href=\"#Random-Simulation\" data-toc-modified-id=\"Random-Simulation-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Random Simulation</a></span></li></ul></li><li><span><a href=\"#Reinforcement-Learning-Introduction\" data-toc-modified-id=\"Reinforcement-Learning-Introduction-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Reinforcement Learning Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Work-Piece-Indicator\" data-toc-modified-id=\"Work-Piece-Indicator-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Work Piece Indicator</a></span></li><li><span><a href=\"#Reinforcement-Learning-in-Cartpole\" data-toc-modified-id=\"Reinforcement-Learning-in-Cartpole-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Reinforcement Learning in Cartpole</a></span></li><li><span><a href=\"#Markov-Decision-Process\" data-toc-modified-id=\"Markov-Decision-Process-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Markov Decision Process</a></span></li><li><span><a href=\"#Deterministic-vs-Stochastic\" data-toc-modified-id=\"Deterministic-vs-Stochastic-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Deterministic vs Stochastic</a></span></li><li><span><a href=\"#Q-Learning\" data-toc-modified-id=\"Q-Learning-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Q Learning</a></span></li></ul></li><li><span><a href=\"#One-Layer-Neural-Nets\" data-toc-modified-id=\"One-Layer-Neural-Nets-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>One Layer Neural Nets</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Layer-Neural-Net-Definition\" data-toc-modified-id=\"One-Layer-Neural-Net-Definition-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>One Layer Neural Net Definition</a></span></li><li><span><a href=\"#BNN-Class\" data-toc-modified-id=\"BNN-Class-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>BNN Class</a></span></li><li><span><a href=\"#BNN_1-Parameters\" data-toc-modified-id=\"BNN_1-Parameters-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>BNN_1 Parameters</a></span></li><li><span><a href=\"#BNN_1-Simulation\" data-toc-modified-id=\"BNN_1-Simulation-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>BNN_1 Simulation</a></span></li><li><span><a href=\"#BNN_2-Parameters\" data-toc-modified-id=\"BNN_2-Parameters-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>BNN_2 Parameters</a></span></li><li><span><a href=\"#BNN_2-simulation\" data-toc-modified-id=\"BNN_2-simulation-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>BNN_2 simulation</a></span></li></ul></li><li><span><a href=\"#Two-Layer-Neural-Nets\" data-toc-modified-id=\"Two-Layer-Neural-Nets-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Two Layer Neural Nets</a></span><ul class=\"toc-item\"><li><span><a href=\"#BNN---TLN-Differences\" data-toc-modified-id=\"BNN---TLN-Differences-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>BNN - TLN Differences</a></span></li><li><span><a href=\"#Two-Layer-Neural-Net-Definition\" data-toc-modified-id=\"Two-Layer-Neural-Net-Definition-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Two Layer Neural Net Definition</a></span></li><li><span><a href=\"#TLN_1-Parameters\" data-toc-modified-id=\"TLN_1-Parameters-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>TLN_1 Parameters</a></span></li><li><span><a href=\"#TLN-Demonstration\" data-toc-modified-id=\"TLN-Demonstration-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>TLN Demonstration</a></span></li><li><span><a href=\"#TLN_1-Simulation\" data-toc-modified-id=\"TLN_1-Simulation-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>TLN_1 Simulation</a></span></li><li><span><a href=\"#TLN_2-Parameters\" data-toc-modified-id=\"TLN_2-Parameters-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>TLN_2 Parameters</a></span></li><li><span><a href=\"#TLN_2-Simulation\" data-toc-modified-id=\"TLN_2-Simulation-9.7\"><span class=\"toc-item-num\">9.7&nbsp;&nbsp;</span>TLN_2 Simulation</a></span></li></ul></li><li><span><a href=\"#Experience-Replay-Network\" data-toc-modified-id=\"Experience-Replay-Network-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Experience Replay Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#TLN---ERN-Differences\" data-toc-modified-id=\"TLN---ERN-Differences-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>TLN - ERN Differences</a></span></li><li><span><a href=\"#Experience-Replay-Class\" data-toc-modified-id=\"Experience-Replay-Class-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Experience Replay Class</a></span></li><li><span><a href=\"#ERN-Class\" data-toc-modified-id=\"ERN-Class-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>ERN Class</a></span></li><li><span><a href=\"#ERN_1-Parameters\" data-toc-modified-id=\"ERN_1-Parameters-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>ERN_1 Parameters</a></span></li><li><span><a href=\"#ERN-Demonstration\" data-toc-modified-id=\"ERN-Demonstration-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;</span>ERN Demonstration</a></span></li><li><span><a href=\"#ERN_1-Simulation\" data-toc-modified-id=\"ERN_1-Simulation-10.6\"><span class=\"toc-item-num\">10.6&nbsp;&nbsp;</span>ERN_1 Simulation</a></span></li><li><span><a href=\"#ERN_2-Parameters\" data-toc-modified-id=\"ERN_2-Parameters-10.7\"><span class=\"toc-item-num\">10.7&nbsp;&nbsp;</span>ERN_2 Parameters</a></span></li><li><span><a href=\"#ERN_2-Simulation\" data-toc-modified-id=\"ERN_2-Simulation-10.8\"><span class=\"toc-item-num\">10.8&nbsp;&nbsp;</span>ERN_2 Simulation</a></span></li></ul></li><li><span><a href=\"#Deep-Q-Network\" data-toc-modified-id=\"Deep-Q-Network-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Deep Q-Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#DQN-Class\" data-toc-modified-id=\"DQN-Class-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>DQN Class</a></span></li><li><span><a href=\"#DQN-1-Parameters\" data-toc-modified-id=\"DQN-1-Parameters-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>DQN-1 Parameters</a></span></li><li><span><a href=\"#DQN-Demonstration\" data-toc-modified-id=\"DQN-Demonstration-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>DQN Demonstration</a></span></li><li><span><a href=\"#DQN-1-Simulation\" data-toc-modified-id=\"DQN-1-Simulation-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;</span>DQN-1 Simulation</a></span></li></ul></li><li><span><a href=\"#Double-DQN-Network\" data-toc-modified-id=\"Double-DQN-Network-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Double DQN Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#DoubleDQN-Class\" data-toc-modified-id=\"DoubleDQN-Class-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>DoubleDQN Class</a></span></li><li><span><a href=\"#DoubleDQN-1-Parameters\" data-toc-modified-id=\"DoubleDQN-1-Parameters-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>DoubleDQN-1 Parameters</a></span></li><li><span><a href=\"#DoubleDQN-1-Simulation\" data-toc-modified-id=\"DoubleDQN-1-Simulation-12.3\"><span class=\"toc-item-num\">12.3&nbsp;&nbsp;</span>DoubleDQN-1 Simulation</a></span></li></ul></li><li><span><a href=\"#DuelingDQN-Nework\" data-toc-modified-id=\"DuelingDQN-Nework-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>DuelingDQN Nework</a></span><ul class=\"toc-item\"><li><span><a href=\"#Three-Layer-Advantage-and-Value-Network-Class\" data-toc-modified-id=\"Three-Layer-Advantage-and-Value-Network-Class-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Three Layer Advantage and Value Network Class</a></span></li><li><span><a href=\"#DuelingDQN-Class\" data-toc-modified-id=\"DuelingDQN-Class-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>DuelingDQN Class</a></span></li><li><span><a href=\"#DuelingDQN-1-Parameters\" data-toc-modified-id=\"DuelingDQN-1-Parameters-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;</span>DuelingDQN-1 Parameters</a></span></li><li><span><a href=\"#DuelingDQN-1-Simulation\" data-toc-modified-id=\"DuelingDQN-1-Simulation-13.4\"><span class=\"toc-item-num\">13.4&nbsp;&nbsp;</span>DuelingDQN-1 Simulation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>The purpose of this notebook is to compare the performance of different neural net architectures, building each of the networks inside of a modular class structure. Episodic performance will be accumulated and later predicted through supervised learning models. This presentation will focus on the code and algorithmic architecture of various models, rather than the mathematics used in the analysis.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Robert Alvarez's 2019 Pytorch Presentation at ODSC: https://github.com/robert-alvarez/pytorch_tutorial\n",
    "1. Code Development Atamis's Udemy Course: https://www.udemy.com/reinforcement-learning-with-pytorch/\n",
    "1. Deep Mind's Papers: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf and https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
    "1. Reinforcement Learning 2018, Sutton & Barto http://incompleteideas.net/book/RLbook2018.pdf\n",
    "1. UCL Course on RL: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:23.773105Z",
     "start_time": "2019-08-05T22:53:23.082008Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from time import time\n",
    "from time import sleep\n",
    "import gym\n",
    "import math\n",
    "\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:23.778764Z",
     "start_time": "2019-08-05T22:53:23.774598Z"
    }
   },
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole v1 Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T09:52:37.550577Z",
     "start_time": "2019-08-05T09:52:37.547720Z"
    }
   },
   "source": [
    "<img src=\"./800px-Cart-pendulum.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Demonstration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:25.006560Z",
     "start_time": "2019-08-05T22:53:23.779972Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "if not hasattr(env.action_space, 'n'):\n",
    "    raise Exception('Keyboard agent only supports discrete action spaces')\n",
    "ACTIONS = env.action_space.n\n",
    "SKIP_CONTROL = 0    # Use previous control decision SKIP_CONTROL times, that's how you\n",
    "                    # can test what skip is still usable.\n",
    "\n",
    "human_agent_action = 0\n",
    "human_wants_restart = False\n",
    "human_sets_pause = False\n",
    "\n",
    "def key_press(key, mod):\n",
    "    global human_agent_action, human_wants_restart, human_sets_pause\n",
    "    if key==0xff0d: human_wants_restart = True\n",
    "    \n",
    "    111#Key 32 is the space key\n",
    "    if key==32: human_sets_pause = not human_sets_pause\n",
    "    a = int( key - ord('0') )\n",
    "    if a <= 0 or a >= ACTIONS: return\n",
    "    human_agent_action = a\n",
    "\n",
    "def key_release(key, mod):\n",
    "    global human_agent_action\n",
    "    a = int( key - ord('0') )\n",
    "    if a <= 0 or a >= ACTIONS: return\n",
    "    if human_agent_action == a:\n",
    "        human_agent_action = 0\n",
    "\n",
    "env.render()\n",
    "env.unwrapped.viewer.window.on_key_press = key_press\n",
    "env.unwrapped.viewer.window.on_key_release = key_release\n",
    "\n",
    "def rollout(env):\n",
    "    global human_agent_action, human_wants_restart, human_sets_pause\n",
    "    human_wants_restart = False\n",
    "    obser = env.reset()\n",
    "    skip = 0\n",
    "    total_reward = 0\n",
    "    total_timesteps = 0\n",
    "    while 1:\n",
    "        if not skip:\n",
    "            print(\"taking action {}\".format(human_agent_action))\n",
    "            print(obser)\n",
    "            a = human_agent_action\n",
    "            total_timesteps += 1\n",
    "            skip = SKIP_CONTROL\n",
    "        else:\n",
    "            skip -= 1\n",
    "\n",
    "        obser, r, done, info = env.step(a)\n",
    "        #if r != 0:\n",
    "            #print(\"reward %0.3f\" % r)\n",
    "        total_reward += r\n",
    "        window_still_open = env.render()\n",
    "        if window_still_open==False: return False\n",
    "        if done: break\n",
    "        if human_wants_restart: break\n",
    "        while human_sets_pause:\n",
    "            env.render()\n",
    "            sleep(0.1)\n",
    "        sleep(0.1)\n",
    "    print(\"timesteps %i reward %0.2f\" % (total_timesteps, total_reward))\n",
    "\n",
    "rollout(env)\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T10:28:02.543750Z",
     "start_time": "2019-08-05T10:28:02.540385Z"
    }
   },
   "source": [
    "# CartPole-v1 Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:25.012588Z",
     "start_time": "2019-08-05T22:53:25.008242Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "inputs = env.observation_space.shape[0]\n",
    "outputs = env.action_space.n\n",
    "\n",
    "print(\"Input Space:\\t{}\\nOutput Space:\\t{}\".format(inputs, outputs))\n",
    "score_to_solve = 450"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action/Input Space\n",
    "\n",
    "|Num|Action|\n",
    "|-|-|\n",
    "|0|Push cart to the left|\n",
    "|1|Push cart to the right|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation/Output Space     \n",
    "\n",
    "\n",
    "|Num|Observation|Min|Max|\n",
    "|-|---------------------|-|-|\n",
    "|0|Cart Position|-4.8|4.8|\n",
    "|1|Cart Velocity|-Inf|Inf|\n",
    "|2|Pole Angle|-24 deg|24 deg|\n",
    "|3|Pole Velocity At Tip|-Inf|Inf|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:25.025879Z",
     "start_time": "2019-08-05T22:53:25.014284Z"
    }
   },
   "outputs": [],
   "source": [
    "def report(report_interval,\n",
    "           episode,\n",
    "           start_time,\n",
    "           steps_total,\n",
    "           epsilon,\n",
    "           frames_total,\n",
    "           solved):\n",
    "    \n",
    "    if episode % report_interval == 0 and episode != 0:\n",
    "        print(\"\"\"\\t\\t\\t *** Episode {} at Total Seconds {:2f}***\n",
    "                 Average Reward for last {} steps: {:2f}\n",
    "                 Average Reward for all steps: {:2f}\n",
    "                 Epsilon: {}, Frames Total :{}\n",
    "                 Solved {} Times\n",
    "              \"\"\".format(episode,\n",
    "                   (time() - start_time),\n",
    "                   str(report_interval),\n",
    "                   sum(steps_total[-report_interval:])/report_interval,\n",
    "                   sum(steps_total)/len(steps_total),\n",
    "                   epsilon,\n",
    "                   frames_total,\n",
    "                   solved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:25.032573Z",
     "start_time": "2019-08-05T22:53:25.027153Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "  \n",
    "    def factory(type, object):\n",
    "        if type == \"Random\":\n",
    "            return Random_Agent(type, object)\n",
    "        elif type == \"BNN\":\n",
    "            return BNN(type, object)\n",
    "        elif type == \"TLN\":\n",
    "            return TLN(type, object)\n",
    "        elif type == \"ERN\":\n",
    "            return ERN(type, object)\n",
    "        elif type == \"ULN\":\n",
    "            return ULN(type, object)\n",
    "        elif type == \"DQN\":\n",
    "            return DQN(type, object)\n",
    "        elif type == \"DoubleDQN\":\n",
    "            return DoubleDQN(type, object)\n",
    "        elif type == \"DuelingDQN\":\n",
    "            return DuelingDQN(type, object)\n",
    "        \n",
    "    factory = staticmethod(factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:25.050648Z",
     "start_time": "2019-08-05T22:53:25.034354Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulation(num_episodes,\n",
    "               agent,\n",
    "               label,\n",
    "               record,\n",
    "               report_interval,\n",
    "               extra_name  = \"\",\n",
    "               mem = False,\n",
    "               show = False):\n",
    "    \n",
    "    #Simulation Initialization\n",
    "    actor = Agent.factory(agent, label)\n",
    "    start_time = time()\n",
    "    solved, frames_total = 0, 0\n",
    "    steps_total = []\n",
    "    \n",
    "    if extra_name != \"\": extra_name = \"_\" + extra_name\n",
    "    \n",
    "    #The recorded dictionary for the simulation\n",
    "    shadow_run = {}\n",
    "    \n",
    "    #Loop through different episodes\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        # Episode Initializations\n",
    "        state = env.reset()\n",
    "        epi_name = actor.name + extra_name + \"_\"+ str(episode).zfill(4)\n",
    "        shadow_run[epi_name] = {} \n",
    "        \n",
    "        #A fancy way to make lists\n",
    "        blanks = [-1 for i in range(500)]\n",
    "        \n",
    "        #Input for episode\n",
    "        actions, steps = [blanks for i in range(2)] \n",
    "        \n",
    "        #Outputs for episodes\n",
    "        cart_position, cart_velocity = [blanks for i in range(2)]\n",
    "        pole_angle, pole_velocity  = [blanks for i in range(2)]\n",
    "        \n",
    "        step = 0\n",
    "                \n",
    "        while True:\n",
    "            #This allows us to watch the model in action\n",
    "            if show == True: rendered = env.render(mode = 'rgb_array')\n",
    "\n",
    "            \n",
    "            #Calculate E-greedy Rate for Exploration vs. Exploitation tradeoff\n",
    "            epsilon = actor.calculate_epsilon(frames_total)\n",
    "\n",
    "            #Actor acts\n",
    "            action = actor.select_action(state, epsilon)\n",
    "\n",
    "            #Record States\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            #Optimize\n",
    "            #Memory Push check\n",
    "            \n",
    "            if mem == True:\n",
    "                memory.push(state, action, new_state, reward, done)\n",
    "                actor.optimize()\n",
    "            else:\n",
    "                actor.optimize(state, action, new_state, reward, done)\n",
    "\n",
    "            #Store Inputs for step\n",
    "            actions[step] = action\n",
    "            \n",
    "            ###\n",
    "            #**DO THIS**\n",
    "            #On update, make a class called Information to clean syntax\n",
    "            ###\n",
    "            \n",
    "            #Unpack Output for step\n",
    "            cart_position[step] = new_state[0]\n",
    "            cart_velocity[step] = new_state[1]\n",
    "            pole_angle[step]    = new_state[2]\n",
    "            pole_velocity[step] = new_state[3]         \n",
    "\n",
    "            if done:\n",
    "                steps_total.append(step)\n",
    "                \n",
    "                ###\n",
    "                #**DO THIS**\n",
    "                #On update, clean up the following syntax\n",
    "                #Also, consider adding number values to the information\n",
    "                ###\n",
    "                \n",
    "                #Below is an overpacked syntax to add a column for each part\n",
    "                #.zfill pads zeros in the front of the string\n",
    "                \n",
    "                for i in range(500):\n",
    "                    D = shadow_run[epi_name]\n",
    "                    D[\"action_{}\".format(str(i).zfill(3))]        = actions[i]\n",
    "                    D[\"cart_position_{}\".format(str(i).zfill(3))] = cart_position[i]\n",
    "                    D[\"cart_velocity_{}\".format(str(i).zfill(3))] = cart_velocity[i]\n",
    "                    D[\"pole_angle_{}\".format(str(i).zfill(3))]    = pole_angle[i]\n",
    "                    D[\"pole_velocity_{}\".format(str(i).zfill(3))] = pole_velocity[i]                                     \n",
    "                \n",
    "                shadow_run[epi_name] = D\n",
    "                #Not needed for this environment\n",
    "                #shadow_run[episode][\"information\"] = information\n",
    "\n",
    "                if step >= score_to_solve: solved += 1\n",
    "\n",
    "                report(report_interval, episode, start_time, steps_total, epsilon, frames_total, solved)\n",
    "\n",
    "                break\n",
    "\n",
    "            step += 1\n",
    "            frames_total += 1\n",
    "            state = new_state\n",
    "        \n",
    "    #Clean up after Episodes\n",
    "    record[actor.name + extra_name] = shadow_run    \n",
    "    env.close()\n",
    "    env.env.close()\n",
    "    \n",
    "    #Plot Results\n",
    "    plot_x = np.linspace(0,num_episodes,num = num_episodes)\n",
    "    \n",
    "    def moving_average(x, w):\n",
    "        return np.convolve(x, np.ones(w), 'valid') / w\n",
    "    \n",
    "    moving_25 = np.append(24*[0], moving_average(np.array(steps_total), 25))\n",
    "    \n",
    "    \n",
    "    fig1, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5));\n",
    "    ax[0].set_title('Performance Plot');\n",
    "    ax[0].scatter(plot_x,\n",
    "                  steps_total,\n",
    "                  c='b',\n",
    "                  #s = 5,\n",
    "                  marker = \"x\",\n",
    "                  label = \"Performance Values\");\n",
    "#     ax[0].plot(plot_x,\n",
    "#                steps_total,\n",
    "#                label = \"Performance Values\",\n",
    "#                linewidth = .5)\n",
    "    ax[0].plot(plot_x, \n",
    "               moving_25,\n",
    "               c = \"orange\",\n",
    "               label = \"25 Session Moving Average\",\n",
    "               linewidth=3.0)\n",
    "    ax[0].legend(loc=\"center right\", bbox_to_anchor=(-.1, 0.5))\n",
    "    \n",
    "    \n",
    "    ax[1].set_title(\"Kernel Density Estimation of Performance\");\n",
    "    sns.kdeplot(np.array(steps_total), shade = True, bw=0.5, ax = ax[1])\n",
    "\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random_Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:25.060581Z",
     "start_time": "2019-08-05T22:53:25.052391Z"
    }
   },
   "outputs": [],
   "source": [
    "class Random_Agent(Agent):\n",
    "    def __init__(self, Agent, label):\n",
    "        self.name = label\n",
    "        \n",
    "    #Randomly chooses an action\n",
    "    def select_action(self,state,epsilon):\n",
    "        action = env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    #No epsilon decision is needed\n",
    "    def calculate_epsilon(self,steps_done):\n",
    "        return 0    \n",
    "    \n",
    "    #No optimization\n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:38.107855Z",
     "start_time": "2019-08-05T22:53:25.062168Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = simulation(50, \"Random\", \"Random_Agent\", {}, 100, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:39.879509Z",
     "start_time": "2019-08-05T22:53:38.109188Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"Random\", \"Random_Agent\", {}, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:39.883481Z",
     "start_time": "2019-08-05T22:53:39.880575Z"
    }
   },
   "outputs": [],
   "source": [
    "len(record[\"Random_Agent\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:40.511450Z",
     "start_time": "2019-08-05T22:53:39.884625Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(record[\"Random_Agent\"]).T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:41.134349Z",
     "start_time": "2019-08-05T22:53:40.512617Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(record[\"Random_Agent\"]).T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Piece Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:41.138491Z",
     "start_time": "2019-08-05T22:53:41.135542Z"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"./WPI.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning in Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T13:05:35.220698Z",
     "start_time": "2019-08-05T13:05:35.217780Z"
    }
   },
   "source": [
    "<img src=\"./WPI.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./MDP.png\">\n",
    "<div style=\"text-align: right\">Source: Wikipedia </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic vs Stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some environments have fixed, finite set of states. These environments are called <strong>Deterministic</strong>. Since the combination of states is along a continuous interval, the following analysis will assume CartPole as a <strong>Stochastic</strong> environment, meaning that it is virtually impossible to know everything about the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the mathematics of Q learning equations are outside of the scope of this presentation, for completeness, they are presented here. <br><br>\n",
    "$${\\displaystyle Q^{new}(s_{t},a_{t})\\leftarrow (1-\\alpha )\\cdot \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {r_{t}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}{\\bigg )}} ^{\\text{learned value}}}$$\n",
    "<br>\n",
    "For more information, please check out http://cs231n.stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Layer Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Layer Neural Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:41.143822Z",
     "start_time": "2019-08-05T22:53:41.139577Z"
    }
   },
   "outputs": [],
   "source": [
    "#Basic One Layer Network\n",
    "class NeuralNetwork_1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #super () Allows us to avoid calling base class\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,number_of_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:41.152648Z",
     "start_time": "2019-08-05T22:53:41.144848Z"
    }
   },
   "outputs": [],
   "source": [
    "class BNN(object):\n",
    "    \n",
    "    def __init__(self, Agent, label):\n",
    "        #Calling Network to Cuda, if available\n",
    "        self.nn = NeuralNetwork_1().to(device)\n",
    "        self.name = label\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        #http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl5.pdf\n",
    "        #Exploitation vs Exploration algorithm\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        #If random > epsilon, then will not attempt random action\n",
    "        if random_for_egreedy > epsilon:      \n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()\n",
    "                \n",
    "        #Otherwise random action is attempted and recorded\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        #Backwards propigation to train network\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            #Prevents Divergence\n",
    "            target_value = reward\n",
    "        else:\n",
    "            #Bellman Equations\n",
    "            new_state_values = self.nn(new_state).detach() #Using detach so we don't update the network\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        #The is where the network calculates the predictions\n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        #The learing between the loss and actual\n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        #Backwards propigate through the netowork\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:41.160584Z",
     "start_time": "2019-08-05T22:53:41.153821Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "#**DO THIS**\n",
    "#Not sure if I like defining the parameters outside of the class like this\n",
    "#Consider reworking in next verson\n",
    "####\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Discount Factor\n",
    "gamma = 0.9995\n",
    "\n",
    "\n",
    "#Exploration vs Exploitation\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.005\n",
    "egreedy_decay = 1000\n",
    "\n",
    "\n",
    "#These lines are useful if I decide to change environments\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:50.596227Z",
     "start_time": "2019-08-05T22:53:41.161783Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"BNN\", \"BNN_1\",record, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:50.600090Z",
     "start_time": "2019-08-05T22:53:50.597429Z"
    }
   },
   "outputs": [],
   "source": [
    "record.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:50.608018Z",
     "start_time": "2019-08-05T22:53:50.601245Z"
    }
   },
   "outputs": [],
   "source": [
    "len(record[\"BNN_1\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_2 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:53:50.613278Z",
     "start_time": "2019-08-05T22:53:50.609177Z"
    }
   },
   "outputs": [],
   "source": [
    "#Change Learning Rate by a factor of TEN\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_2 simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:54:03.305200Z",
     "start_time": "2019-08-05T22:53:50.614480Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"BNN\",\"BNN_2\", record, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:54:03.312886Z",
     "start_time": "2019-08-05T22:54:03.309378Z"
    }
   },
   "outputs": [],
   "source": [
    "record.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN - TLN Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>The only major difference between these two networks is the addition of an extra linear layer in the neural net</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Layer Neural Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:54:03.322585Z",
     "start_time": "2019-08-05T22:54:03.315277Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_2, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer,number_of_outputs)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        #Adding activation function between layers\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:54:03.343531Z",
     "start_time": "2019-08-05T22:54:03.324547Z"
    }
   },
   "outputs": [],
   "source": [
    "class TLN(object):\n",
    "    \n",
    "    def __init__(self,Agent, label):\n",
    "        self.nn = NeuralNetwork_2().to(device)\n",
    "        self.name = label\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN_1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:54:03.351233Z",
     "start_time": "2019-08-05T22:54:03.344788Z"
    }
   },
   "outputs": [],
   "source": [
    "#Reset Learning Rate and Gamma\n",
    "learning_rate = 0.001\n",
    "gamma = 0.9995\n",
    "\n",
    "#Now need a Hidden Layer for NN\n",
    "hidden_layer = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:54:03.356877Z",
     "start_time": "2019-08-05T22:54:03.352633Z"
    }
   },
   "outputs": [],
   "source": [
    "#_ = simulation(100, \"TLN\",\"TLN_1\", record, 100, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN_1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:56:54.133109Z",
     "start_time": "2019-08-05T22:54:03.358821Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"TLN\",\"TLN_1\", record, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN_2 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:56:54.136345Z",
     "start_time": "2019-08-05T22:56:54.134270Z"
    }
   },
   "outputs": [],
   "source": [
    "#This time we will explore a different gamma\n",
    "gamma = 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN_2 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:57:11.959226Z",
     "start_time": "2019-08-05T22:56:54.137361Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"TLN\",\"TLN_2\", record, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN - ERN Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>According to the paper <span style=\"text-decoration:underline\">Playing Atari with Deep Reinforcement Learning</span>, two main problems arise from simple neural nets in RL problems. First, there is a delay between action and rewards; these problems are thus \"particularly daunting when compared to the direct association between inputs and targets found in supervised learning.\" Second, \"most deep\n",
    "learning algorithms assume the data samples to be independent, while in reinforcement learning one\n",
    "typically encounters sequences of highly correlated states.\" In other words, the data distributions are non stationary.<br><br>\n",
    "Deep Mind's solution to this problem was to use: \"an experience replay mechanism which randomly samples previous transitions, and thereby smooths the training distribution over many past behaviors.\" These samples are taken in batches, not sequentially.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T19:42:45.890861Z",
     "start_time": "2019-08-05T19:42:45.886940Z"
    }
   },
   "source": [
    "<img src=\"./DQN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>In terms of our Pytorch Algorithm, This will involve two major improvements:\n",
    "1. The addition of the Experience Replay Class\n",
    "1. Pushing all memory inside of tensors</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:57:11.965292Z",
     "start_time": "2019-08-05T22:57:11.960466Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "     \n",
    "    #Now Training is batches to avoid the clustering and oversampling\n",
    "    #Of past training\n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        #Checks to see if memory ready to be filled\n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        #aggregates elements based on iterables, here it is making\n",
    "        #batches of experiences\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "    \n",
    "    #Verify Memory size\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:57:12.021342Z",
     "start_time": "2019-08-05T22:57:11.966620Z"
    }
   },
   "outputs": [],
   "source": [
    "class ERN(object):\n",
    "    def __init__(self, Agent, label):\n",
    "        self.nn = NeuralNetwork_2().to(device)\n",
    "        self.name = label\n",
    "        \n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        new_state_values = self.nn(new_state).detach()\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN_1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:57:12.029121Z",
     "start_time": "2019-08-05T22:57:12.022640Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.025\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_mem_size = 500000\n",
    "batch_size = 32\n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T22:57:12.036706Z",
     "start_time": "2019-08-05T22:57:12.030366Z"
    }
   },
   "outputs": [],
   "source": [
    "#_ = simulation(25, \"ERN\",\"ERN_1\", record, 100, mem = True, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN_1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:03:45.398976Z",
     "start_time": "2019-08-05T22:57:12.038028Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"ERN\",\"ERN_1\", record, 100, mem = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN_2 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:03:45.503602Z",
     "start_time": "2019-08-05T23:03:45.400195Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's explore a new learning rate and discount factor with TLN\n",
    "learning_rate = 0.05\n",
    "\n",
    "#Discount Factor\n",
    "gamma = 0.95\n",
    "\n",
    "#Also Let's test the effect of quadrupling nn size\n",
    "hidden_layer = 256\n",
    "\n",
    "#Try a larger batch size\n",
    "batch_size = 64\n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN_2 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:10:06.116341Z",
     "start_time": "2019-08-05T23:03:45.505206Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"ERN\",\"ERN_2\", record, 100, mem = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:10:06.127992Z",
     "start_time": "2019-08-05T23:10:06.117521Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self, Agent, label):\n",
    "        self.name = label\n",
    "        \n",
    "        self.nn = NeuralNetwork_2().to(device)\n",
    "        self.target_nn = NeuralNetwork_2().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        new_state_values = self.target_nn(new_state).detach()\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN-1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:10:06.141855Z",
     "start_time": "2019-08-05T23:10:06.129159Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.025\n",
    "gamma = .99\n",
    "\n",
    "hidden_layer = 128\n",
    "\n",
    "#This effects how often our other neural net updates the first network\n",
    "update_target_frequency = 250\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "clip_error = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:10:06.147300Z",
     "start_time": "2019-08-05T23:10:06.142978Z"
    }
   },
   "outputs": [],
   "source": [
    "#_ = simulation(100, \"DQN\",\"DQN_1\", record, 100, mem = True, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN-1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:17:00.787652Z",
     "start_time": "2019-08-05T23:10:06.148528Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"DQN\",\"DQN_1\", record, 100, mem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:17:00.793038Z",
     "start_time": "2019-08-05T23:17:00.789434Z"
    }
   },
   "outputs": [],
   "source": [
    "record.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DoubleDQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:17:00.808338Z",
     "start_time": "2019-08-05T23:17:00.794747Z"
    }
   },
   "outputs": [],
   "source": [
    "class DoubleDQN(object):\n",
    "    def __init__(self, Agent, label):\n",
    "        self.name = label\n",
    "        \n",
    "        self.nn = NeuralNetwork_2().to(device)\n",
    "        self.target_nn = NeuralNetwork_2().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        #These states tend to over estimate the best action, so in Double\n",
    "        #We will try something else\n",
    "        #new_state_values = self.target_nn(new_state).detach()\n",
    "        #max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "\n",
    "        #Find the index of the max, with the target, but calculate with \n",
    "        #Learning network        \n",
    "        new_state_indexes = self.nn(new_state).detach()\n",
    "        max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "\n",
    "        new_state_values = self.target_nn(new_state).detach()\n",
    "        max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DoubleDQN-1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:17:00.815976Z",
     "start_time": "2019-08-05T23:17:00.809870Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using All of the Same Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DoubleDQN-1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:23:54.896334Z",
     "start_time": "2019-08-05T23:17:00.817323Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"DoubleDQN\",\"DoubleDQN_1\", record, 100, mem = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuelingDQN Nework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Layer Advantage and Value Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:23:54.904361Z",
     "start_time": "2019-08-05T23:23:54.897947Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork_AandV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_AandV, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        \n",
    "        self.advantage = nn.Linear(hidden_layer,hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        \n",
    "        self.value = nn.Linear(hidden_layer,hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer,1)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        \n",
    "        output_advantage = self.advantage(output1)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value(output1)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "\n",
    "        return output_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DuelingDQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:23:54.915649Z",
     "start_time": "2019-08-05T23:23:54.905601Z"
    }
   },
   "outputs": [],
   "source": [
    "class DuelingDQN(object):\n",
    "    def __init__(self, Agent, label):\n",
    "        self.name = label\n",
    "        \n",
    "        #The only real change is rework the networks\n",
    "        self.nn = NeuralNetwork_AandV().to(device)\n",
    "        self.target_nn = NeuralNetwork_AandV().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "    \n",
    "        new_state_indexes = self.nn(new_state).detach()\n",
    "        max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "\n",
    "        new_state_values = self.target_nn(new_state).detach()\n",
    "        max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DuelingDQN-1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T23:23:54.923239Z",
     "start_time": "2019-08-05T23:23:54.916844Z"
    }
   },
   "outputs": [],
   "source": [
    "#Same as Above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DuelingDQN-1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-05T22:53:23.315Z"
    }
   },
   "outputs": [],
   "source": [
    "record = simulation(1000, \"DuelingDQN\",\"DuelingDQN_1\", record, 100, mem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
