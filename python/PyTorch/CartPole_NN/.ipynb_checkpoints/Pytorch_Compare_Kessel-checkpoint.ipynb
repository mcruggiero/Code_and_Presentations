{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Notes</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-Imports\" data-toc-modified-id=\"Library-Imports-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Library Imports</a></span></li><li><span><a href=\"#Cuda-Support\" data-toc-modified-id=\"Cuda-Support-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Cuda Support</a></span></li></ul></li><li><span><a href=\"#CartPole-v1-Environment\" data-toc-modified-id=\"CartPole-v1-Environment-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>CartPole v1 Environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Action/Input-Space\" data-toc-modified-id=\"Action/Input-Space-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Action/Input Space</a></span></li><li><span><a href=\"#Observation/Output-Space\" data-toc-modified-id=\"Observation/Output-Space-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Observation/Output Space</a></span></li></ul></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Informational-Functions\" data-toc-modified-id=\"Informational-Functions-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Informational Functions</a></span></li><li><span><a href=\"#Agent-Factory\" data-toc-modified-id=\"Agent-Factory-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Agent Factory</a></span></li><li><span><a href=\"#Simulation-Function\" data-toc-modified-id=\"Simulation-Function-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Simulation Function</a></span></li></ul></li><li><span><a href=\"#Random-Agent-Class\" data-toc-modified-id=\"Random-Agent-Class-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Random Agent Class</a></span></li><li><span><a href=\"#One-Layer-Neural-Nets\" data-toc-modified-id=\"One-Layer-Neural-Nets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>One Layer Neural Nets</a></span><ul class=\"toc-item\"><li><span><a href=\"#BNN-Neural-Net-Definition\" data-toc-modified-id=\"BNN-Neural-Net-Definition-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>BNN Neural Net Definition</a></span></li><li><span><a href=\"#BNN-Class\" data-toc-modified-id=\"BNN-Class-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>BNN Class</a></span></li><li><span><a href=\"#BNN_1-Parameters\" data-toc-modified-id=\"BNN_1-Parameters-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>BNN_1 Parameters</a></span></li><li><span><a href=\"#BNN_1-Simulation\" data-toc-modified-id=\"BNN_1-Simulation-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>BNN_1 Simulation</a></span></li><li><span><a href=\"#BNN_2-Parameters\" data-toc-modified-id=\"BNN_2-Parameters-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>BNN_2 Parameters</a></span></li><li><span><a href=\"#BNN_2-simulation\" data-toc-modified-id=\"BNN_2-simulation-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>BNN_2 simulation</a></span></li></ul></li><li><span><a href=\"#Two-Layer-Neural-Nets\" data-toc-modified-id=\"Two-Layer-Neural-Nets-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Two Layer Neural Nets</a></span><ul class=\"toc-item\"><li><span><a href=\"#TLN-With-Tanh,-Neural-Net-Definition\" data-toc-modified-id=\"TLN-With-Tanh,-Neural-Net-Definition-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>TLN With Tanh, Neural Net Definition</a></span></li><li><span><a href=\"#TLN_1-Parameters\" data-toc-modified-id=\"TLN_1-Parameters-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>TLN_1 Parameters</a></span></li><li><span><a href=\"#TLN_1-Simulation\" data-toc-modified-id=\"TLN_1-Simulation-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>TLN_1 Simulation</a></span></li><li><span><a href=\"#TLN_2-Parameters\" data-toc-modified-id=\"TLN_2-Parameters-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>TLN_2 Parameters</a></span></li></ul></li><li><span><a href=\"#Experience-Replay-Network\" data-toc-modified-id=\"Experience-Replay-Network-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Experience Replay Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Experience-Replay-Class\" data-toc-modified-id=\"Experience-Replay-Class-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Experience Replay Class</a></span></li><li><span><a href=\"#ERN-Class\" data-toc-modified-id=\"ERN-Class-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>ERN Class</a></span></li><li><span><a href=\"#ERN_1-Parameters\" data-toc-modified-id=\"ERN_1-Parameters-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>ERN_1 Parameters</a></span></li><li><span><a href=\"#ERN_1-Simulation\" data-toc-modified-id=\"ERN_1-Simulation-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>ERN_1 Simulation</a></span></li><li><span><a href=\"#ERN_2-Parameters\" data-toc-modified-id=\"ERN_2-Parameters-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>ERN_2 Parameters</a></span></li><li><span><a href=\"#ERN_2-Simulation\" data-toc-modified-id=\"ERN_2-Simulation-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>ERN_2 Simulation</a></span></li></ul></li><li><span><a href=\"#ULN\" data-toc-modified-id=\"ULN-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>ULN</a></span></li><li><span><a href=\"#Random-Cartpole\" data-toc-modified-id=\"Random-Cartpole-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Random Cartpole</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>This code is inspired by Atamai AI Team's Udemy course and Robert Alvarez's 2019 Pytorch presentation at ODSC.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:47:52.630733Z",
     "start_time": "2019-08-02T20:47:52.624236Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from time import time\n",
    "import gym\n",
    "import math\n",
    "\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:27:33.388536Z",
     "start_time": "2019-08-02T20:27:33.380793Z"
    }
   },
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole v1 Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:44:52.657799Z",
     "start_time": "2019-08-02T18:44:52.623640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Space:\t4\n",
      "Output Space:\t2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "inputs = env.observation_space.shape[0]\n",
    "outputs = env.action_space.n\n",
    "\n",
    "print(\"Input Space:\\t{}\\nOutput Space:\\t{}\".format(inputs, outputs))\n",
    "score_to_solve = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action/Input Space\n",
    "\n",
    "|Num|Action|\n",
    "|-|-|\n",
    "|0|Push cart to the left|\n",
    "|1|Push cart to the right|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation/Output Space     \n",
    "\n",
    "\n",
    "|Num|Observation|Min|Max|\n",
    "|-|---------------------|-|-|\n",
    "|0|Cart Position|-4.8|4.8|\n",
    "|1|Cart Velocity|-Inf|Inf|\n",
    "|2|Pole Angle|-24 deg|24 deg|\n",
    "|3|Pole Velocity At Tip|-Inf|Inf|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informational Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:44:54.426205Z",
     "start_time": "2019-08-02T18:44:54.417087Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_information(steps_total, shadow_run):\n",
    "    e_list = []\n",
    "    for i in shadow_run:\n",
    "        for j in shadow_run[i][\"epsilon\"]:\n",
    "            e_list.append(j)\n",
    "    \n",
    "    #s = list(range(sum(steps_total)))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,10), nrows=2, ncols=1)\n",
    "    fig.subplots_adjust(hspace=.25)\n",
    "\n",
    "    ax[0].set(title=\"Rewards\",\n",
    "              xlabel='Session', \n",
    "              ylabel='Steps')\n",
    "    ax[0].bar(torch.arange(len(steps_total)), \n",
    "            steps_total, \n",
    "            alpha=0.6, \n",
    "            color='green',\n",
    "            width= 1)\n",
    "    \n",
    "    ax[1].set(title=\"Epsilon\",\n",
    "          xlabel='Steps', \n",
    "          ylabel='Session')\n",
    "    ax[1].plot(e_list)\n",
    "    plt.show()\n",
    "    \n",
    "def report(report_interval,\n",
    "           episode,\n",
    "           start_time,\n",
    "           steps_total,\n",
    "           epsilon,\n",
    "           frames_total,\n",
    "           solved):\n",
    "    \n",
    "    if episode % report_interval == 0 and episode != 0:\n",
    "        print(\"\"\"\\t\\t\\t *** Episode {} at Total Seconds {:2f}***\n",
    "                 Average Reward for last {} steps: {:2f}\n",
    "                 Average Reward for all steps: {:2f}\n",
    "                 Epsilon: {}, Frames Total :{}\n",
    "                 Solved {} Times\n",
    "              \"\"\".format(episode,\n",
    "                   (time() - start_time),\n",
    "                   str(report_interval),\n",
    "                   sum(steps_total[-report_interval:])/report_interval,\n",
    "                   sum(steps_total)/len(steps_total),\n",
    "                   epsilon,\n",
    "                   frames_total,\n",
    "                   solved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:26:00.909838Z",
     "start_time": "2019-08-02T20:26:00.903861Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "  \n",
    "    def factory(type, object):\n",
    "        if type == \"Random\":\n",
    "            return Random_Agent(type, object)\n",
    "        elif type == \"BNN\":\n",
    "            return BNN(type, object)\n",
    "        elif type == \"TLN\":\n",
    "            return TLN(type, object)\n",
    "        elif type == \"ERN\":\n",
    "            return ERN(type, object)\n",
    "        elif type == \"ULN\":\n",
    "            return ULN(type, object)\n",
    "        \n",
    "    factory = staticmethod(factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:47:14.669246Z",
     "start_time": "2019-08-02T20:47:14.655260Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulation(num_episodes, agent, label ,record, report_interval, extra_name  = \"\", mem = False):\n",
    "    #Simulation Initialization\n",
    "    actor = Agent.factory(agent, label)\n",
    "    start_time = time()\n",
    "    solved, frames_total = 0, 0\n",
    "    steps_total = []\n",
    "    \n",
    "    if extra_name != \"\": extra_name = \"_\" + extra_name\n",
    "    \n",
    "    #The recorded dictionary for the simulation\n",
    "    shadow_run = {}\n",
    "    \n",
    "    #Loop through different episodes\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        # Episode Initializations\n",
    "        state = env.reset()\n",
    "        epi_name = actor.name + extra_name + \"_\"+ str(episode).zfill(4)\n",
    "        shadow_run[epi_name] = {} \n",
    "        \n",
    "        #A facny way to make lists\n",
    "        blanks = [-1 for i in range(500)]\n",
    "        \n",
    "        #Input for episode\n",
    "        actions, steps = [blanks for i in range(2)] \n",
    "        \n",
    "        #Outputs for episodes\n",
    "        cart_position, cart_velocity = [blanks for i in range(2)]\n",
    "        pole_angle, pole_velocity  = [blanks for i in range(2)]\n",
    "        \n",
    "        step = 0\n",
    "                \n",
    "        while True:\n",
    "            \n",
    "            #Calculate Learning Rate\n",
    "            epsilon = actor.calculate_epsilon(frames_total)\n",
    "\n",
    "            #Actor acts\n",
    "            action = actor.select_action(state, epsilon)\n",
    "\n",
    "            #Record States\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            #Optimize\n",
    "            #Memory Push check\n",
    "            \n",
    "            if mem == True:\n",
    "                memory.push(state, action, new_state, reward, done)\n",
    "                actor.optimize()\n",
    "            else:\n",
    "                actor.optimize(state, action, new_state, reward, done)\n",
    "\n",
    "            #Store Inputs for step\n",
    "            actions[step] = action\n",
    "            \n",
    "            ###\n",
    "            #**DO THIS**\n",
    "            #On update, make a class called Information to clean syntax\n",
    "            ###\n",
    "            \n",
    "            #Unpack Output for step\n",
    "            cart_position[step] = new_state[0]\n",
    "            cart_velocity[step] = new_state[1]\n",
    "            pole_angle[step] = new_state[2]\n",
    "            pole_velocity[step] = new_state[3]         \n",
    "\n",
    "            if done:\n",
    "                steps_total.append(step)\n",
    "                \n",
    "                ###\n",
    "                #**DO THIS**\n",
    "                #On update, clean up the following syntax\n",
    "                ###\n",
    "                \n",
    "                #Below is an overpacked syntax to add a column for each part\n",
    "                #.zfill pads zeros in the front of the string\n",
    "                \n",
    "                for i in range(500):\n",
    "                    D = shadow_run[epi_name]\n",
    "                    D[\"action_{}\".format(str(i).zfill(3))]        = actions[i]\n",
    "                    D[\"cart_position_{}\".format(str(i).zfill(3))] = cart_position[i]\n",
    "                    D[\"cart_velocity_{}\".format(str(i).zfill(3))] = cart_velocity[i]\n",
    "                    D[\"pole_angle_{}\".format(str(i).zfill(3))]    = pole_angle[i]\n",
    "                    D[\"pole_velocity_{}\".format(str(i).zfill(3))] = pole_velocity[i]                                     \n",
    "                \n",
    "                shadow_run[epi_name] = D\n",
    "                #Not needed for this environment\n",
    "                #shadow_run[episode][\"information\"] = information\n",
    "\n",
    "                if step >= score_to_solve: solved += 1\n",
    "\n",
    "                report(report_interval, episode, start_time, steps_total, epsilon, frames_total, solved)\n",
    "\n",
    "                break\n",
    "\n",
    "            step += 1\n",
    "            frames_total += 1\n",
    "            state = new_state\n",
    "        \n",
    "    #Clean up after Episodes\n",
    "    record[actor.name + extra_name] = shadow_run    \n",
    "    env.close()\n",
    "    env.env.close()\n",
    "\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:47:15.448295Z",
     "start_time": "2019-08-02T20:47:15.442575Z"
    }
   },
   "outputs": [],
   "source": [
    "class Random_Agent(Agent):\n",
    "    def __init__(self, Agent, label):\n",
    "        self.name = label\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        action = env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def calculate_epsilon(self,steps_done):\n",
    "        return 0    \n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:47:16.974329Z",
     "start_time": "2019-08-02T20:47:15.598239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 0.145662***\n",
      "                 Average Reward for last 100 steps: 22.380000\n",
      "                 Average Reward for all steps: 22.306931\n",
      "                 Epsilon: 0, Frames Total :2253\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 0.269686***\n",
      "                 Average Reward for last 100 steps: 20.930000\n",
      "                 Average Reward for all steps: 21.621891\n",
      "                 Epsilon: 0, Frames Total :4346\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 0.400549***\n",
      "                 Average Reward for last 100 steps: 21.430000\n",
      "                 Average Reward for all steps: 21.558140\n",
      "                 Epsilon: 0, Frames Total :6489\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 0.528595***\n",
      "                 Average Reward for last 100 steps: 22.940000\n",
      "                 Average Reward for all steps: 21.902743\n",
      "                 Epsilon: 0, Frames Total :8783\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 0.657146***\n",
      "                 Average Reward for last 100 steps: 23.830000\n",
      "                 Average Reward for all steps: 22.287425\n",
      "                 Epsilon: 0, Frames Total :11166\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 0.784400***\n",
      "                 Average Reward for last 100 steps: 21.950000\n",
      "                 Average Reward for all steps: 22.231281\n",
      "                 Epsilon: 0, Frames Total :13361\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 0.927035***\n",
      "                 Average Reward for last 100 steps: 21.610000\n",
      "                 Average Reward for all steps: 22.142653\n",
      "                 Epsilon: 0, Frames Total :15522\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 1.071021***\n",
      "                 Average Reward for last 100 steps: 21.440000\n",
      "                 Average Reward for all steps: 22.054931\n",
      "                 Epsilon: 0, Frames Total :17666\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 1.213878***\n",
      "                 Average Reward for last 100 steps: 21.300000\n",
      "                 Average Reward for all steps: 21.971143\n",
      "                 Epsilon: 0, Frames Total :19796\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"Random\", \"Random_Agent\", {}, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:44:59.067754Z",
     "start_time": "2019-08-02T18:44:59.054745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(record[\"Random_Agent\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:44:59.671812Z",
     "start_time": "2019-08-02T18:44:59.068914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['action_000', 'action_001', 'action_002', 'action_003', 'action_004',\n",
       "       'action_005', 'action_006', 'action_007', 'action_008', 'action_009',\n",
       "       ...\n",
       "       'pole_velocity_490', 'pole_velocity_491', 'pole_velocity_492',\n",
       "       'pole_velocity_493', 'pole_velocity_494', 'pole_velocity_495',\n",
       "       'pole_velocity_496', 'pole_velocity_497', 'pole_velocity_498',\n",
       "       'pole_velocity_499'],\n",
       "      dtype='object', length=2500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(record[\"Random_Agent\"]).T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:00.240577Z",
     "start_time": "2019-08-02T18:44:59.673280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_000</th>\n",
       "      <th>action_001</th>\n",
       "      <th>action_002</th>\n",
       "      <th>action_003</th>\n",
       "      <th>action_004</th>\n",
       "      <th>action_005</th>\n",
       "      <th>action_006</th>\n",
       "      <th>action_007</th>\n",
       "      <th>action_008</th>\n",
       "      <th>action_009</th>\n",
       "      <th>...</th>\n",
       "      <th>pole_velocity_490</th>\n",
       "      <th>pole_velocity_491</th>\n",
       "      <th>pole_velocity_492</th>\n",
       "      <th>pole_velocity_493</th>\n",
       "      <th>pole_velocity_494</th>\n",
       "      <th>pole_velocity_495</th>\n",
       "      <th>pole_velocity_496</th>\n",
       "      <th>pole_velocity_497</th>\n",
       "      <th>pole_velocity_498</th>\n",
       "      <th>pole_velocity_499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random_Agent_0000</th>\n",
       "      <td>-0.295034</td>\n",
       "      <td>-0.596013</td>\n",
       "      <td>-0.313771</td>\n",
       "      <td>-0.035451</td>\n",
       "      <td>0.240786</td>\n",
       "      <td>0.516792</td>\n",
       "      <td>0.794408</td>\n",
       "      <td>0.490517</td>\n",
       "      <td>0.191459</td>\n",
       "      <td>0.480765</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_Agent_0001</th>\n",
       "      <td>0.265604</td>\n",
       "      <td>0.550591</td>\n",
       "      <td>0.252050</td>\n",
       "      <td>0.542271</td>\n",
       "      <td>0.834093</td>\n",
       "      <td>1.129317</td>\n",
       "      <td>0.844514</td>\n",
       "      <td>1.151713</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.615849</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_Agent_0002</th>\n",
       "      <td>-0.290968</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-0.295400</td>\n",
       "      <td>-0.005871</td>\n",
       "      <td>-0.303514</td>\n",
       "      <td>-0.015911</td>\n",
       "      <td>0.269737</td>\n",
       "      <td>-0.029914</td>\n",
       "      <td>-0.327902</td>\n",
       "      <td>-0.040809</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_Agent_0003</th>\n",
       "      <td>-0.311033</td>\n",
       "      <td>-0.591818</td>\n",
       "      <td>-0.289597</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>-0.279498</td>\n",
       "      <td>-0.567943</td>\n",
       "      <td>-0.272828</td>\n",
       "      <td>0.018715</td>\n",
       "      <td>0.308525</td>\n",
       "      <td>0.013115</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_Agent_0004</th>\n",
       "      <td>0.328969</td>\n",
       "      <td>0.629653</td>\n",
       "      <td>0.347316</td>\n",
       "      <td>0.653812</td>\n",
       "      <td>0.377894</td>\n",
       "      <td>0.106367</td>\n",
       "      <td>-0.162598</td>\n",
       "      <td>0.152657</td>\n",
       "      <td>0.466970</td>\n",
       "      <td>0.782147</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   action_000  action_001  action_002  action_003  action_004  \\\n",
       "Random_Agent_0000   -0.295034   -0.596013   -0.313771   -0.035451    0.240786   \n",
       "Random_Agent_0001    0.265604    0.550591    0.252050    0.542271    0.834093   \n",
       "Random_Agent_0002   -0.290968    0.000407   -0.295400   -0.005871   -0.303514   \n",
       "Random_Agent_0003   -0.311033   -0.591818   -0.289597    0.009003   -0.279498   \n",
       "Random_Agent_0004    0.328969    0.629653    0.347316    0.653812    0.377894   \n",
       "\n",
       "                   action_005  action_006  action_007  action_008  action_009  \\\n",
       "Random_Agent_0000    0.516792    0.794408    0.490517    0.191459    0.480765   \n",
       "Random_Agent_0001    1.129317    0.844514    1.151713    0.879857    0.615849   \n",
       "Random_Agent_0002   -0.015911    0.269737   -0.029914   -0.327902   -0.040809   \n",
       "Random_Agent_0003   -0.567943   -0.272828    0.018715    0.308525    0.013115   \n",
       "Random_Agent_0004    0.106367   -0.162598    0.152657    0.466970    0.782147   \n",
       "\n",
       "                   ...  pole_velocity_490  pole_velocity_491  \\\n",
       "Random_Agent_0000  ...               -1.0               -1.0   \n",
       "Random_Agent_0001  ...               -1.0               -1.0   \n",
       "Random_Agent_0002  ...               -1.0               -1.0   \n",
       "Random_Agent_0003  ...               -1.0               -1.0   \n",
       "Random_Agent_0004  ...               -1.0               -1.0   \n",
       "\n",
       "                   pole_velocity_492  pole_velocity_493  pole_velocity_494  \\\n",
       "Random_Agent_0000               -1.0               -1.0               -1.0   \n",
       "Random_Agent_0001               -1.0               -1.0               -1.0   \n",
       "Random_Agent_0002               -1.0               -1.0               -1.0   \n",
       "Random_Agent_0003               -1.0               -1.0               -1.0   \n",
       "Random_Agent_0004               -1.0               -1.0               -1.0   \n",
       "\n",
       "                   pole_velocity_495  pole_velocity_496  pole_velocity_497  \\\n",
       "Random_Agent_0000               -1.0               -1.0               -1.0   \n",
       "Random_Agent_0001               -1.0               -1.0               -1.0   \n",
       "Random_Agent_0002               -1.0               -1.0               -1.0   \n",
       "Random_Agent_0003               -1.0               -1.0               -1.0   \n",
       "Random_Agent_0004               -1.0               -1.0               -1.0   \n",
       "\n",
       "                   pole_velocity_498  pole_velocity_499  \n",
       "Random_Agent_0000               -1.0               -1.0  \n",
       "Random_Agent_0001               -1.0               -1.0  \n",
       "Random_Agent_0002               -1.0               -1.0  \n",
       "Random_Agent_0003               -1.0               -1.0  \n",
       "Random_Agent_0004               -1.0               -1.0  \n",
       "\n",
       "[5 rows x 2500 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(record[\"Random_Agent\"]).T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Layer Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN Neural Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:02.570078Z",
     "start_time": "2019-08-02T18:45:02.567047Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork_1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,number_of_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:04.273048Z",
     "start_time": "2019-08-02T18:45:04.265732Z"
    }
   },
   "outputs": [],
   "source": [
    "class BNN(object):\n",
    "    \n",
    "    def __init__(self, Agent, label):\n",
    "        self.nn = NeuralNetwork_1().to(device)\n",
    "        self.name = label\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:05.475228Z",
     "start_time": "2019-08-02T18:45:05.470112Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "#**DO THIS**\n",
    "#Not sure if I like defining the parameters outside of the class like this\n",
    "#Consider reworking in next verson\n",
    "####\n",
    "\n",
    "learning_rate = 0.025\n",
    "gamma = 0.85\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.025\n",
    "egreedy_decay = 500\n",
    "\n",
    "\n",
    "#These lines are useful if I decide to change environments\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:16.493669Z",
     "start_time": "2019-08-02T18:45:06.454621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 0.615495***\n",
      "                 Average Reward for last 100 steps: 14.490000\n",
      "                 Average Reward for all steps: 14.613861\n",
      "                 Epsilon: 0.07070573975292152, Frames Total :1476\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 1.344678***\n",
      "                 Average Reward for last 100 steps: 19.110000\n",
      "                 Average Reward for all steps: 16.850746\n",
      "                 Epsilon: 0.026000223911422038, Frames Total :3387\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 2.374798***\n",
      "                 Average Reward for last 100 steps: 28.350000\n",
      "                 Average Reward for all steps: 20.671096\n",
      "                 Epsilon: 0.025003448637292522, Frames Total :6222\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 3.294754***\n",
      "                 Average Reward for last 100 steps: 23.900000\n",
      "                 Average Reward for all steps: 21.476309\n",
      "                 Epsilon: 0.025000028954755147, Frames Total :8612\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 4.507333***\n",
      "                 Average Reward for last 100 steps: 32.860000\n",
      "                 Average Reward for all steps: 23.748503\n",
      "                 Epsilon: 0.025000000040507606, Frames Total :11898\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 5.656838***\n",
      "                 Average Reward for last 100 steps: 30.870000\n",
      "                 Average Reward for all steps: 24.933444\n",
      "                 Epsilon: 0.025000000000084375, Frames Total :14985\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 6.916489***\n",
      "                 Average Reward for last 100 steps: 31.840000\n",
      "                 Average Reward for all steps: 25.918688\n",
      "                 Epsilon: 0.025000000000000147, Frames Total :18169\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 8.161830***\n",
      "                 Average Reward for last 100 steps: 31.390000\n",
      "                 Average Reward for all steps: 26.601748\n",
      "                 Epsilon: 0.025, Frames Total :21308\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 9.319664***\n",
      "                 Average Reward for last 100 steps: 31.130000\n",
      "                 Average Reward for all steps: 27.104329\n",
      "                 Epsilon: 0.025, Frames Total :24421\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"BNN\", \"BNN_1\",record, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:16.497473Z",
     "start_time": "2019-08-02T18:45:16.494818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Random_Agent', 'BNN_1'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:16.512767Z",
     "start_time": "2019-08-02T18:45:16.498939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(record[\"BNN_1\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_2 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:23.765883Z",
     "start_time": "2019-08-02T18:45:23.763158Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.025\n",
    "gamma = 0.85\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.001\n",
    "egreedy_decay = 1000\n",
    "\n",
    "\n",
    "#These lines are useful if I decide to change environments\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN_2 simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:35.975522Z",
     "start_time": "2019-08-02T18:45:24.906301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 0.756378***\n",
      "                 Average Reward for last 100 steps: 21.550000\n",
      "                 Average Reward for all steps: 21.485149\n",
      "                 Epsilon: 0.10364567760284202, Frames Total :2170\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 1.624185***\n",
      "                 Average Reward for last 100 steps: 24.040000\n",
      "                 Average Reward for all steps: 22.756219\n",
      "                 Epsilon: 0.010274632958209785, Frames Total :4574\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 2.905789***\n",
      "                 Average Reward for last 100 steps: 34.570000\n",
      "                 Average Reward for all steps: 26.681063\n",
      "                 Epsilon: 0.001292375318264808, Frames Total :8031\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 3.992270***\n",
      "                 Average Reward for last 100 steps: 30.480000\n",
      "                 Average Reward for all steps: 27.628429\n",
      "                 Epsilon: 0.0010138743014643545, Frames Total :11079\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 5.121746***\n",
      "                 Average Reward for last 100 steps: 29.530000\n",
      "                 Average Reward for all steps: 28.007984\n",
      "                 Epsilon: 0.0010007240015928382, Frames Total :14032\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 6.257096***\n",
      "                 Average Reward for last 100 steps: 30.890000\n",
      "                 Average Reward for all steps: 28.487521\n",
      "                 Epsilon: 0.0010000329764474308, Frames Total :17121\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 7.451295***\n",
      "                 Average Reward for last 100 steps: 31.800000\n",
      "                 Average Reward for all steps: 28.960057\n",
      "                 Epsilon: 0.00100000137134717, Frames Total :20301\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 8.578934***\n",
      "                 Average Reward for last 100 steps: 29.390000\n",
      "                 Average Reward for all steps: 29.013733\n",
      "                 Epsilon: 0.001000000072569801, Frames Total :23240\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 9.833433***\n",
      "                 Average Reward for last 100 steps: 33.160000\n",
      "                 Average Reward for all steps: 29.473918\n",
      "                 Epsilon: 0.0010000000026341193, Frames Total :26556\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"BNN\",\"BNN_2\", record, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:35.979870Z",
     "start_time": "2019-08-02T18:45:35.976886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Random_Agent', 'BNN_1', 'BNN_2'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN With Tanh, Neural Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:40.407699Z",
     "start_time": "2019-08-02T18:45:40.404214Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_2, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer,number_of_outputs)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:46.175858Z",
     "start_time": "2019-08-02T18:45:46.168869Z"
    }
   },
   "outputs": [],
   "source": [
    "class TLN(object):\n",
    "    \n",
    "    def __init__(self,Agent, label):\n",
    "        self.nn = NeuralNetwork_2().to(device)\n",
    "        self.name = label\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN_1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:45:47.688786Z",
     "start_time": "2019-08-02T18:45:47.681269Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN_1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:48:36.714035Z",
     "start_time": "2019-08-02T18:45:50.182179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 5.326541***\n",
      "                 Average Reward for last 100 steps: 106.740000\n",
      "                 Average Reward for all steps: 105.782178\n",
      "                 Epsilon: 0.02000000046182579, Frames Total :10684\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 18.492555***\n",
      "                 Average Reward for last 100 steps: 269.710000\n",
      "                 Average Reward for all steps: 187.338308\n",
      "                 Epsilon: 0.02, Frames Total :37655\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 35.015273***\n",
      "                 Average Reward for last 100 steps: 299.300000\n",
      "                 Average Reward for all steps: 224.534884\n",
      "                 Epsilon: 0.02, Frames Total :67585\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 51.425258***\n",
      "                 Average Reward for last 100 steps: 302.760000\n",
      "                 Average Reward for all steps: 244.042394\n",
      "                 Epsilon: 0.02, Frames Total :97861\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 73.080202***\n",
      "                 Average Reward for last 100 steps: 382.220000\n",
      "                 Average Reward for all steps: 271.622754\n",
      "                 Epsilon: 0.02, Frames Total :136083\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 88.664135***\n",
      "                 Average Reward for last 100 steps: 276.540000\n",
      "                 Average Reward for all steps: 272.440932\n",
      "                 Epsilon: 0.02, Frames Total :163737\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 108.290298***\n",
      "                 Average Reward for last 100 steps: 365.350000\n",
      "                 Average Reward for all steps: 285.694722\n",
      "                 Epsilon: 0.02, Frames Total :200272\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 127.275387***\n",
      "                 Average Reward for last 100 steps: 353.170000\n",
      "                 Average Reward for all steps: 294.118602\n",
      "                 Epsilon: 0.02, Frames Total :235589\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 145.675921***\n",
      "                 Average Reward for last 100 steps: 344.300000\n",
      "                 Average Reward for all steps: 299.688124\n",
      "                 Epsilon: 0.02, Frames Total :270019\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"TLN\",\"TLN_1\", record, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLN_2 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:03:48.700523Z",
     "start_time": "2019-08-02T19:03:48.696540Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 128\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.0002\n",
    "egreedy_decay = 1000\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:08:17.819814Z",
     "start_time": "2019-08-02T19:03:55.381548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 7.614553***\n",
      "                 Average Reward for last 100 steps: 136.030000\n",
      "                 Average Reward for all steps: 134.851485\n",
      "                 Epsilon: 0.0002010940952821291, Frames Total :13620\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 33.257410***\n",
      "                 Average Reward for last 100 steps: 432.200000\n",
      "                 Average Reward for all steps: 282.786070\n",
      "                 Epsilon: 0.0002, Frames Total :56840\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 56.948570***\n",
      "                 Average Reward for last 100 steps: 353.100000\n",
      "                 Average Reward for all steps: 306.146179\n",
      "                 Epsilon: 0.0002, Frames Total :92150\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 85.565067***\n",
      "                 Average Reward for last 100 steps: 431.950000\n",
      "                 Average Reward for all steps: 337.518703\n",
      "                 Epsilon: 0.0002, Frames Total :135345\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 112.520384***\n",
      "                 Average Reward for last 100 steps: 405.340000\n",
      "                 Average Reward for all steps: 351.055888\n",
      "                 Epsilon: 0.0002, Frames Total :175879\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 141.772844***\n",
      "                 Average Reward for last 100 steps: 424.780000\n",
      "                 Average Reward for all steps: 363.322795\n",
      "                 Epsilon: 0.0002, Frames Total :218357\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 173.286834***\n",
      "                 Average Reward for last 100 steps: 462.380000\n",
      "                 Average Reward for all steps: 377.453638\n",
      "                 Epsilon: 0.0002, Frames Total :264595\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 204.782315***\n",
      "                 Average Reward for last 100 steps: 472.740000\n",
      "                 Average Reward for all steps: 389.349563\n",
      "                 Epsilon: 0.0002, Frames Total :311869\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 231.976021***\n",
      "                 Average Reward for last 100 steps: 405.420000\n",
      "                 Average Reward for all steps: 391.133185\n",
      "                 Epsilon: 0.0002, Frames Total :352411\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"TLN\",\"TLN_2\", record, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:47:23.914693Z",
     "start_time": "2019-08-02T20:47:23.906296Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:48:00.095440Z",
     "start_time": "2019-08-02T20:48:00.082298Z"
    }
   },
   "outputs": [],
   "source": [
    "class ERN(object):\n",
    "    def __init__(self, Agent, label):\n",
    "        self.nn = NeuralNetwork_2().to(device)\n",
    "        self.name = label\n",
    "        \n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        new_state_values = self.nn(new_state).detach()\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN_1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:48:00.946453Z",
     "start_time": "2019-08-02T20:48:00.940921Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_mem_size = 500000\n",
    "batch_size = 32\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.00001\n",
    "egreedy_decay = 1000\n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN_1 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:54:48.189723Z",
     "start_time": "2019-08-02T20:48:01.606110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 34.907120***\n",
      "                 Average Reward for last 100 steps: 271.120000\n",
      "                 Average Reward for all steps: 268.673267\n",
      "                 Epsilon: 1.0000001476463187e-05, Frames Total :27136\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 72.296592***\n",
      "                 Average Reward for last 100 steps: 381.940000\n",
      "                 Average Reward for all steps: 325.024876\n",
      "                 Epsilon: 1e-05, Frames Total :65330\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 115.393419***\n",
      "                 Average Reward for last 100 steps: 476.580000\n",
      "                 Average Reward for all steps: 375.375415\n",
      "                 Epsilon: 1e-05, Frames Total :112988\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 160.132879***\n",
      "                 Average Reward for last 100 steps: 493.460000\n",
      "                 Average Reward for all steps: 404.822943\n",
      "                 Epsilon: 1e-05, Frames Total :162334\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 205.246898***\n",
      "                 Average Reward for last 100 steps: 494.720000\n",
      "                 Average Reward for all steps: 422.766467\n",
      "                 Epsilon: 1e-05, Frames Total :211806\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 241.475458***\n",
      "                 Average Reward for last 100 steps: 450.390000\n",
      "                 Average Reward for all steps: 427.362729\n",
      "                 Epsilon: 1e-05, Frames Total :256845\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 283.169323***\n",
      "                 Average Reward for last 100 steps: 449.810000\n",
      "                 Average Reward for all steps: 430.564907\n",
      "                 Epsilon: 1e-05, Frames Total :301826\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 322.136915***\n",
      "                 Average Reward for last 100 steps: 481.190000\n",
      "                 Average Reward for all steps: 436.885144\n",
      "                 Epsilon: 1e-05, Frames Total :349945\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 362.224102***\n",
      "                 Average Reward for last 100 steps: 453.110000\n",
      "                 Average Reward for all steps: 438.685905\n",
      "                 Epsilon: 1e-05, Frames Total :395256\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"ERN\",\"ERN_1\", record, 100, mem = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN_2 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T20:58:27.562391Z",
     "start_time": "2019-08-02T20:58:27.464605Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 128\n",
    "\n",
    "replay_mem_size = 500000\n",
    "batch_size = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.00001\n",
    "egreedy_decay = 1000\n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERN_2 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T21:07:28.934750Z",
     "start_time": "2019-08-02T20:58:59.512195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 31.809440***\n",
      "                 Average Reward for last 100 steps: 306.910000\n",
      "                 Average Reward for all steps: 304.039604\n",
      "                 Epsilon: 1.0000000041488024e-05, Frames Total :30708\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 75.131952***\n",
      "                 Average Reward for last 100 steps: 419.560000\n",
      "                 Average Reward for all steps: 361.512438\n",
      "                 Epsilon: 1e-05, Frames Total :72664\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 119.230043***\n",
      "                 Average Reward for last 100 steps: 465.320000\n",
      "                 Average Reward for all steps: 396.000000\n",
      "                 Epsilon: 1e-05, Frames Total :119196\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 172.078404***\n",
      "                 Average Reward for last 100 steps: 485.230000\n",
      "                 Average Reward for all steps: 418.251870\n",
      "                 Epsilon: 1e-05, Frames Total :167719\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 226.571254***\n",
      "                 Average Reward for last 100 steps: 482.380000\n",
      "                 Average Reward for all steps: 431.051896\n",
      "                 Epsilon: 1e-05, Frames Total :215957\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 282.206789***\n",
      "                 Average Reward for last 100 steps: 484.480000\n",
      "                 Average Reward for all steps: 439.941764\n",
      "                 Epsilon: 1e-05, Frames Total :264405\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 339.818019***\n",
      "                 Average Reward for last 100 steps: 496.790000\n",
      "                 Average Reward for all steps: 448.051355\n",
      "                 Epsilon: 1e-05, Frames Total :314084\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 396.156585***\n",
      "                 Average Reward for last 100 steps: 490.260000\n",
      "                 Average Reward for all steps: 453.320849\n",
      "                 Epsilon: 1e-05, Frames Total :363110\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 452.552657***\n",
      "                 Average Reward for last 100 steps: 491.510000\n",
      "                 Average Reward for all steps: 457.559378\n",
      "                 Epsilon: 1e-05, Frames Total :412261\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"ERN\",\"ERN_1\", record, 100, mem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ULN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:57:02.225527Z",
     "start_time": "2019-08-02T18:57:02.203736Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_3, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear3 = nn.Linear(hidden_layer,number_of_outputs)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        output2 = self.activation(output2)\n",
    "        output3 = self.linear3(output2)\n",
    "        \n",
    "        return output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:57:02.651102Z",
     "start_time": "2019-08-02T18:57:02.630548Z"
    }
   },
   "outputs": [],
   "source": [
    "class ULN(object):\n",
    "    \n",
    "    def __init__(self,Agent, label):\n",
    "        self.nn = NeuralNetwork_3().to(device)\n",
    "        self.name = label\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def calculate_epsilon(self, steps_done):\n",
    "        epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "                  math.exp(-1. * steps_done / egreedy_decay )\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T18:58:54.275627Z",
     "start_time": "2019-08-02T18:57:22.988706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t *** Episode 100 at Total Seconds 5.510469***\n",
      "                 Average Reward for last 100 steps: 91.420000\n",
      "                 Average Reward for all steps: 90.900990\n",
      "                 Epsilon: 0.020000009331842396, Frames Total :9181\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 200 at Total Seconds 10.977631***\n",
      "                 Average Reward for last 100 steps: 78.890000\n",
      "                 Average Reward for all steps: 84.925373\n",
      "                 Epsilon: 0.020000000000001312, Frames Total :17070\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 300 at Total Seconds 23.593939***\n",
      "                 Average Reward for last 100 steps: 187.860000\n",
      "                 Average Reward for all steps: 119.122924\n",
      "                 Epsilon: 0.02, Frames Total :35856\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 400 at Total Seconds 31.083823***\n",
      "                 Average Reward for last 100 steps: 94.690000\n",
      "                 Average Reward for all steps: 113.029925\n",
      "                 Epsilon: 0.02, Frames Total :45325\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 500 at Total Seconds 39.017373***\n",
      "                 Average Reward for last 100 steps: 99.980000\n",
      "                 Average Reward for all steps: 110.425150\n",
      "                 Epsilon: 0.02, Frames Total :55323\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 600 at Total Seconds 46.034800***\n",
      "                 Average Reward for last 100 steps: 85.890000\n",
      "                 Average Reward for all steps: 106.342762\n",
      "                 Epsilon: 0.02, Frames Total :63912\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 700 at Total Seconds 58.152692***\n",
      "                 Average Reward for last 100 steps: 150.950000\n",
      "                 Average Reward for all steps: 112.706134\n",
      "                 Epsilon: 0.02, Frames Total :79007\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 800 at Total Seconds 71.259990***\n",
      "                 Average Reward for last 100 steps: 159.040000\n",
      "                 Average Reward for all steps: 118.490637\n",
      "                 Epsilon: 0.02, Frames Total :94911\n",
      "                 Solved 0 Times\n",
      "              \n",
      "\t\t\t *** Episode 900 at Total Seconds 81.779123***\n",
      "                 Average Reward for last 100 steps: 127.000000\n",
      "                 Average Reward for all steps: 119.435072\n",
      "                 Epsilon: 0.02, Frames Total :107611\n",
      "                 Solved 0 Times\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "record = simulation(1000, \"ULN\",\"ULN_1\", record, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "gamma = 0.99\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.001\n",
    "egreedy_decay = num_episodes * 5\n",
    "report_interval = 10\n",
    "\n",
    "hidden_layer = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_agent = QNet_Agent()\n",
    "start_time = time()\n",
    "solved, frames_total = 0, 0\n",
    "steps_total = []\n",
    "shadow_run = {}\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Episode Initializations\n",
    "    state = env.reset()\n",
    "    shadow_run[episode] = {}\n",
    "    actions, states, shots = [], [], []\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "\n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        rendered = env.render(mode = 'rgb_array')\n",
    "\n",
    "        # Agent Optimize\n",
    "        qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "\n",
    "        # Store states for step\n",
    "        actions.append(action)\n",
    "        states.append(new_state)\n",
    "        shots.append(rendered)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            shadow_run[episode][\"actions\"] = actions\n",
    "            shadow_run[episode][\"states\"] = states\n",
    "            shadow_run[episode][\"shots\"] = shots\n",
    "            shadow_run[episode][\"steps\"] = step\n",
    "\n",
    "            if step >= score_to_solve: solved += 1\n",
    "\n",
    "            report(report_interval, episode, start_time, steps_total, epsilon, frames_total, solved)\n",
    "\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n",
    "\n",
    "plot_information(steps_total,shadow_run)\n",
    "display_frames(num_episodes-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T12:00:24.742649Z",
     "start_time": "2019-08-02T12:00:24.736764Z"
    }
   },
   "outputs": [],
   "source": [
    "class Car(object):\n",
    "  \n",
    "    def factory(type):\n",
    "        if type == \"Racecar\":\n",
    "            return Racecar(type)\n",
    "        if type == \"Van\":\n",
    "            return Van(type)\n",
    "\n",
    "    factory = staticmethod(factory)\n",
    "\n",
    "class Racecar(Car):\n",
    "    \n",
    "    def __init__(self,Car):\n",
    "        self.name = Car\n",
    "        \n",
    "    def drive(self):\n",
    "        print(\"Racecar driving.\")\n",
    "\n",
    "class Van(Car):\n",
    "    def drive(self):\n",
    "        print(\"Van driving.\")\n",
    "\n",
    "# Create object using factory.\n",
    "obj = Car.factory(\"Racecar\")\n",
    "obj.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object,name):\n",
    "    def __init__(self,name):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        if self.name == \"random\":\n",
    "        else:\n",
    "            self.nn = NeuralNetwork().to(device)\n",
    "            self.loss_func = nn.MSELoss()\n",
    "            #self.loss_func = nn.SmoothL1Loss()\n",
    "            self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "            #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T09:44:59.822975Z",
     "start_time": "2019-08-02T09:44:58.353174Z"
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "steps_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()    \n",
    "    step = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        #Random Step\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            steps_total.append(step)\n",
    "            print(\"Episode finished after %i steps\" % step )\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green')\n",
    "plt.show()\n",
    "\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T12:35:02.044747Z",
     "start_time": "2019-08-02T12:35:01.770782Z"
    }
   },
   "outputs": [],
   "source": [
    "test[\"Random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:00:22.875011Z",
     "start_time": "2019-08-02T13:00:22.871560Z"
    }
   },
   "outputs": [],
   "source": [
    "blanks = [-200 for i in range(500)]\n",
    "actions, states, information = [blanks for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:00:25.183021Z",
     "start_time": "2019-08-02T13:00:25.175250Z"
    }
   },
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:09:15.718114Z",
     "start_time": "2019-08-02T13:09:15.712287Z"
    }
   },
   "outputs": [],
   "source": [
    "bob = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:09:23.871441Z",
     "start_time": "2019-08-02T13:09:23.865264Z"
    }
   },
   "outputs": [],
   "source": [
    "bob[2] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T13:09:25.363473Z",
     "start_time": "2019-08-02T13:09:25.358178Z"
    }
   },
   "outputs": [],
   "source": [
    "bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T15:23:29.940524Z",
     "start_time": "2019-08-02T15:23:29.934723Z"
    }
   },
   "outputs": [],
   "source": [
    "str(54).zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
